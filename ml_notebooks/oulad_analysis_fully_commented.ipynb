{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procrastination Prediction using OULAD Dataset\n",
    "\n",
    "**Author:** Jeremiah Agbaje  \n",
    "**Date:** February 2026\n",
    "\n",
    "**Purpose of this notebook:**\n",
    "This notebook implements a complete pipeline for predicting student procrastination risk using transfer learning.\n",
    "We'll pre-train a Bi-LSTM model on the Open University Learning Analytics Dataset (OULAD), which contains\n",
    "behavioral data from 32,593 students. This pre-trained model can then be fine-tuned on smaller institutional\n",
    "datasets to enable procrastination prediction even when local data is limited.\n",
    "\n",
    "**The procrastination prediction problem:**\n",
    "Online students often struggle with self-regulation, leading to late submissions, irregular study patterns,\n",
    "and eventual course dropout. By identifying at-risk students early (within the first few weeks of a course),\n",
    "we can trigger timely interventions like Mental Contrasting with Implementation Intentions (MCII) to help\n",
    "students develop better study habits before procrastination becomes a detrimental pattern.\n",
    "\n",
    "**Why transfer learning?**\n",
    "Most institutions don't have years of historical student data to train accurate prediction models. Transfer\n",
    "learning allows us to leverage patterns learned from OULAD's large dataset and adapt them to new contexts\n",
    "with just a few hundred local students, dramatically reducing the data requirements for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, we'll install and import all the libraries we need for this project. Each library serves a specific purpose\n",
    "in our data science pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries for data manipulation, visualization, and deep learning\n",
    "# The -q flag makes the installation quiet (less output)\n",
    "# We're installing:\n",
    "#   - pandas: For working with tabular data (like spreadsheets)\n",
    "#   - numpy: For numerical computations and arrays\n",
    "#   - matplotlib & seaborn: For creating visualizations\n",
    "#   - scikit-learn: For traditional machine learning algorithms and preprocessing\n",
    "#   - tensorflow: For building and training neural networks (our Bi-LSTM model)\n",
    "#   - statsmodels: For time series analysis (ACF/PACF plots, stationarity tests)\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn tensorflow statsmodels -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core data science libraries\n",
    "import pandas as pd  # DataFrame operations - think of it like Excel in Python\n",
    "import numpy as np  # Numerical operations - fast array math\n",
    "import matplotlib.pyplot as plt  # Basic plotting\n",
    "import seaborn as sns  # Beautiful statistical visualizations\n",
    "from datetime import datetime  # For handling dates/times if needed\n",
    "import warnings  # To suppress annoying warning messages\n",
    "\n",
    "# Suppress warnings to keep output clean\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the plotting style to make our graphs look professional\n",
    "# 'seaborn-v0_8-darkgrid' gives us a nice grid background for easier reading\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Use the 'husl' color palette - provides visually distinct colors for different categories\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scikit-learn tools for preprocessing and traditional machine learning\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "# StandardScaler: Normalizes features to have mean=0 and std=1\n",
    "#   - Why? Neural networks train better when all features are on the same scale\n",
    "#   - Example: 'total_clicks' might be 0-10000, but 'late_rate' is 0-1\n",
    "#   - Scaling makes them comparable\n",
    "# LabelEncoder: Converts text labels (\"Low\", \"Medium\", \"High\") to numbers (0, 1, 2)\n",
    "#   - Why? Neural networks need numeric outputs, not strings\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "# KMeans: Unsupervised clustering algorithm\n",
    "#   - We use this to automatically group students into Low/Medium/High risk categories\n",
    "#   - It finds natural patterns in the data without being told what to look for\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_test_split: Divides data into training and testing sets\n",
    "#   - Training set: Model learns patterns from this (80% of data)\n",
    "#   - Test set: We evaluate model performance on unseen data (20%)\n",
    "#   - Why? To ensure our model generalizes to new students, not just memorizing training data\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score\n",
    "# classification_report: Shows precision, recall, F1-score for each class\n",
    "# confusion_matrix: Shows where model makes mistakes (predicts High when actual is Medium, etc.)\n",
    "# silhouette_score: Measures how well-separated clusters are (for K-means evaluation)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# PCA: Principal Component Analysis - reduces dimensionality while preserving variance\n",
    "#   - We have 4 features, but visualizing in 4D is impossible\n",
    "#   - PCA projects data into 2D so we can visualize clusters on a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow and Keras for building neural networks\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Sequential: A linear stack of layers\n",
    "#   - Our model architecture: Input → Bi-LSTM → Dense → Output\n",
    "#   - Sequential means data flows through layers in order\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\n",
    "# LSTM: Long Short-Term Memory layer - the core of our sequence model\n",
    "#   - Designed to remember patterns over time (e.g., procrastination building over weeks)\n",
    "#   - Has \"memory gates\" that decide what information to keep or forget\n",
    "# Bidirectional: Wraps LSTM to process sequences forward AND backward\n",
    "#   - Why? Past behavior (days 1-5) affects future, but we also see patterns looking back\n",
    "#   - Gives model richer context\n",
    "# Dense: Fully connected layer - traditional neural network layer\n",
    "#   - Every neuron connects to every neuron in previous layer\n",
    "# Dropout: Randomly turns off neurons during training\n",
    "#   - Why? Prevents overfitting by forcing model to not rely on any single neuron\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# EarlyStopping: Stops training if validation loss stops improving\n",
    "#   - Saves time and prevents overfitting\n",
    "#   - Example: If loss doesn't improve for 10 epochs, stop training\n",
    "# ModelCheckpoint: Saves best model during training\n",
    "#   - Keeps the weights from the epoch with lowest validation loss\n",
    "\n",
    "# Print TensorFlow version and check for GPU availability\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# GPU context: Training neural networks is computationally expensive\n",
    "# GPUs can parallelize matrix operations, making training 10-100x faster\n",
    "# If GPU shows [], we're using CPU (slower but still works for this dataset size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statsmodels for time series analysis\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "# plot_acf: Autocorrelation Function plot\n",
    "#   - Shows correlation between a time series and its lagged versions\n",
    "#   - Helps us understand: \"Does today's behavior predict tomorrow's?\"\n",
    "#   - If ACF shows strong correlation at lag 7, students have weekly patterns\n",
    "# plot_pacf: Partial Autocorrelation Function plot\n",
    "#   - Like ACF but removes indirect correlations\n",
    "#   - Helps determine optimal sequence length for LSTM\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "# adfuller: Augmented Dickey-Fuller test for stationarity\n",
    "#   - Stationarity means statistical properties (mean, variance) don't change over time\n",
    "#   - Why it matters: Non-stationary data (like stock prices with trends) is harder to model\n",
    "#   - If p-value < 0.05, the time series is stationary (good for LSTM)\n",
    "#   - If p-value >= 0.05, data has trends/seasonality that need to be removed first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from Google Drive\n",
    "\n",
    "OULAD consists of multiple CSV files that need to be joined together to create our training dataset.\n",
    "We'll load them from Google Drive where they've been uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to access files\n",
    "# This connects your Google Drive to the Colab runtime\n",
    "# You'll be prompted to authenticate and grant permission\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# After mounting, your Google Drive appears as a folder at /content/drive/MyDrive/\n",
    "# You can then access any files you've uploaded to your Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your OULAD data folder\n",
    "# IMPORTANT: Update this path to match where YOU stored the OULAD CSV files in your Drive\n",
    "BASE_PATH = '/content/drive/MyDrive/ALU Capstone/OULAD_data/'\n",
    "\n",
    "# Why organize this way?\n",
    "# By storing the base path in a variable, if you move your data folder,\n",
    "# you only need to update ONE line instead of every file loading line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OULAD datasets from CSV files\n",
    "# Each CSV represents a different aspect of student behavior\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# 1. studentVle.csv - The behavioral goldmine (10+ million rows)\n",
    "#    Contains every click interaction students made with course materials\n",
    "#    Columns: student_id, course_module, date, site_id (resource clicked), sum_click (how many clicks)\n",
    "#    This is where we'll extract procrastination signals:\n",
    "#      - Irregular study patterns (some days 0 clicks, other days 500+)\n",
    "#      - Last-minute cramming (activity spikes near deadlines)\n",
    "#      - Study gaps (long periods of inactivity)\n",
    "student_vle = pd.read_csv(BASE_PATH + 'studentVle.csv')\n",
    "\n",
    "# 2. studentAssessment.csv - Submission behavior (~173k rows)\n",
    "#    Records when students submitted assignments and their scores\n",
    "#    Columns: student_id, assessment_id, date_submitted, score\n",
    "#    Critical for calculating:\n",
    "#      - Late submission rate\n",
    "#      - Average days late\n",
    "#      - Whether student even submitted (missing submissions = extreme procrastination)\n",
    "student_assessment = pd.read_csv(BASE_PATH + 'studentAssessment.csv')\n",
    "\n",
    "# 3. studentInfo.csv - Student demographics and outcomes (~32k rows)\n",
    "#    Contains student characteristics and final course results\n",
    "#    Columns: student_id, gender, age_band, education_level, final_result (Pass/Fail/Withdraw)\n",
    "#    We use this to:\n",
    "#      - Identify unique students\n",
    "#      - Link students to their specific course enrollments\n",
    "#      - (Optional) Could use final_result as validation: do predicted procrastinators fail more?\n",
    "student_info = pd.read_csv(BASE_PATH + 'studentInfo.csv')\n",
    "\n",
    "# 4. assessments.csv - Assignment details (~206 rows)\n",
    "#    Metadata about each assignment: when it's due, what type, how much it's worth\n",
    "#    Columns: assessment_id, course_module, assessment_type (TMA/CMA/Exam), date (deadline), weight\n",
    "#    CRITICAL for procrastination detection:\n",
    "#      - We need deadlines to calculate \"days late\"\n",
    "#      - We need deadlines to measure \"last-minute\" behavior (clicks in week before deadline)\n",
    "#      - Without this, we can't distinguish \"late work\" from \"finished early\"\n",
    "assessments = pd.read_csv(BASE_PATH + 'assessments.csv')\n",
    "\n",
    "# 5. vle.csv - Course resource metadata\n",
    "#    Describes what each clickable resource is (video, quiz, forum, etc.)\n",
    "#    We might not use this directly, but it's good to have for deeper analysis\n",
    "vle = pd.read_csv(BASE_PATH + 'vle.csv')\n",
    "\n",
    "# Print dataset sizes to verify successful loading\n",
    "print(f\"VLE interactions: {len(student_vle):,}\")  # Expect ~10 million\n",
    "print(f\"Assessment submissions: {len(student_assessment):,}\")  # Expect ~173k\n",
    "print(f\"Unique students: {len(student_info):,}\")  # Expect ~32k\n",
    "\n",
    "# Context: OULAD represents 7 courses over 4 semesters (2013-2014)\n",
    "# Courses: AAA, BBB, CCC, DDD, EEE, FFF, GGG (anonymized names)\n",
    "# Semesters: 2013B, 2013J, 2014B, 2014J (B=February, J=October)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Before we start building features, let's understand what data we're working with.\n",
    "This is like getting to know your ingredients before cooking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. studentVle.csv - The Behavioral Goldmine\n",
    "\n",
    "**What it contains:** Every single click/interaction students made with the Virtual Learning Environment (VLE)\n",
    "\n",
    "**Columns:**\n",
    "- `code_module`: Course identifier (e.g., AAA, BBB)\n",
    "- `code_presentation`: Semester (e.g., 2013J for 2013 January)\n",
    "- `id_student`: Unique student ID\n",
    "- `id_site`: Specific VLE resource (video, quiz, forum)\n",
    "- `date`: Day number relative to course start (Day 0 = first day of class, Day -30 = 30 days before start)\n",
    "- `sum_click`: Number of clicks on that resource that day\n",
    "\n",
    "**Why this matters for procrastination:**\n",
    "- **Irregular patterns:** Procrastinators study inconsistently - 0 clicks some days, 500+ others\n",
    "- **Last-minute cramming:** Spikes in activity right before deadlines\n",
    "- **Long gaps:** Days or weeks without any activity, then sudden re-engagement\n",
    "- **Low overall engagement:** Total clicks much lower than peers\n",
    "\n",
    "**Example interpretation:**\n",
    "If Student X has clicks: `[5, 0, 0, 0, 0, 0, 150]` over a week, that's a classic procrastination pattern.\n",
    "If Student Y has clicks: `[20, 25, 18, 22, 30, 19, 28]`, that's consistent engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of VLE interaction data\n",
    "student_vle.head()\n",
    "\n",
    "# What to look for in this output:\n",
    "# - Are dates negative? (means activity before official start, like previewing materials)\n",
    "# - Are sum_click values reasonable? (typically 1-100 per day, but can spike to 500+)\n",
    "# - Are there many rows per student? (active students have thousands of rows, disengaged have few)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. studentAssessment.csv - Submission Behavior\n",
    "\n",
    "**What it contains:** Student submissions for assignments/exams\n",
    "\n",
    "**Columns:**\n",
    "- `id_assessment`: Assignment ID (links to assessments.csv for metadata)\n",
    "- `id_student`: Student ID\n",
    "- `date_submitted`: When they submitted (day number)\n",
    "- `is_banked`: Whether they used previous credit (rare, usually False)\n",
    "- `score`: Grade received (0-100)\n",
    "\n",
    "**Why this matters for procrastination:**\n",
    "The KEY is comparing `date_submitted` (when they actually turned it in) to the `date` in assessments.csv (the deadline):\n",
    "- If `date_submitted - deadline > 0`: **Late submission** (procrastination signal)\n",
    "- If `date_submitted - deadline < 0`: Submitted early (good self-regulation)\n",
    "- If `date_submitted` is missing: **Never submitted** (extreme procrastination or dropout)\n",
    "\n",
    "**Critical feature we'll create:**\n",
    "- `late_rate`: What percentage of their assignments did they submit late?\n",
    "  - 0% = Always on time (low risk)\n",
    "  - 50% = Half late (medium risk)\n",
    "  - 100% = Always late (high risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of assessment submission data\n",
    "student_assessment.head()\n",
    "\n",
    "# What to check:\n",
    "# - Do date_submitted values make sense? (usually positive, between 0 and 365)\n",
    "# - Are scores present? (some might be NaN if assignment not graded yet)\n",
    "# - Notice: This table alone doesn't tell us if submission is late - we need to join with assessments.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. studentInfo.csv - Demographics and Outcomes\n",
    "\n",
    "**What it contains:** Student characteristics and final course results\n",
    "\n",
    "**Columns:**\n",
    "- `id_student`: Student ID\n",
    "- `code_module`, `code_presentation`: Course/semester\n",
    "- `gender`, `region`, `highest_education`, `age_band`: Demographics\n",
    "- `num_of_prev_attempts`: Have they taken this course before? (0 = first time)\n",
    "- `studied_credits`: Workload (number of credits)\n",
    "- `disability`: Whether student reported a disability\n",
    "- `final_result`: **Pass** / **Fail** / **Withdrawn** / **Distinction**\n",
    "\n",
    "**Why we need this:**\n",
    "- **Uniqueness:** Ensures one row per student per course enrollment\n",
    "- **Sampling:** We'll use this to select students for faster processing during development\n",
    "- **Validation:** We can check if predicted high-risk procrastinators actually failed/withdrew more often\n",
    "\n",
    "**Note on demographics:** We're NOT using demographics (gender, age) as features for procrastination prediction.\n",
    "Why? Ethical concerns - we want predictions based on behavior, not student characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of student info\n",
    "student_info.head()\n",
    "\n",
    "# Notice: A single student can appear multiple times if they took multiple courses\n",
    "# Example: Student 123 in AAA-2013J and BBB-2014B would be 2 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. assessments.csv - Assignment Deadlines\n",
    "\n",
    "**What it contains:** Information about each assignment\n",
    "\n",
    "**Columns:**\n",
    "- `id_assessment`: Unique assignment ID\n",
    "- `code_module`: Which course this assignment belongs to\n",
    "- `code_presentation`: Which semester\n",
    "- `assessment_type`: \n",
    "  - **TMA** (Tutor-Marked Assignment): Like homework, graded by instructor\n",
    "  - **CMA** (Computer-Marked Assignment): Like quizzes, auto-graded\n",
    "  - **Exam**: Final exam\n",
    "- `date`: **THE DEADLINE** - day number when assignment is due\n",
    "- `weight`: Percentage of final grade (e.g., 10% means worth 10 points out of 100)\n",
    "\n",
    "**Why deadlines are CRITICAL:**\n",
    "Without knowing deadlines, we can't calculate:\n",
    "1. **Late submissions:** Was `date_submitted` after the deadline?\n",
    "2. **Last-minute behavior:** Did student cram in the 7 days before deadline?\n",
    "3. **Deadline-driven patterns:** Some students only work when deadlines approach\n",
    "\n",
    "**How we'll use this:**\n",
    "We'll JOIN this table with studentAssessment to calculate:\n",
    "```python\n",
    "days_late = date_submitted - deadline\n",
    "if days_late > 0:\n",
    "    # Assignment submitted late\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of assessment metadata\n",
    "assessments.head()\n",
    "\n",
    "# Note: This table has only ~206 rows because it's one row per unique assignment\n",
    "# But thousands of students submit each assignment, so studentAssessment.csv has ~173k rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in our two key behavioral datasets\n",
    "# Missing data can break our feature engineering if not handled properly\n",
    "\n",
    "print(\"Missing values in student_vle:\")\n",
    "print(student_vle.isnull().sum())\n",
    "# Expected: Should be 0 missing values in all columns\n",
    "# If any column has missing values, we need to decide:\n",
    "#   - Drop those rows?\n",
    "#   - Fill with 0 or mean?\n",
    "#   - Use a different imputation strategy?\n",
    "\n",
    "print(\"\\nMissing values in student_assessment:\")\n",
    "print(student_assessment.isnull().sum())\n",
    "# Note: 'score' might have missing values if assignments aren't graded yet\n",
    "# That's okay - we're focused on submission timing, not grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data for Faster Processing\n",
    "\n",
    "**The challenge:** OULAD has 32,593 students, which means millions of rows of interaction data.\n",
    "Processing all of it takes hours. During development and proof-of-concept, we want fast iteration.\n",
    "\n",
    "**The solution:** Sample a subset of students (5,000) for rapid prototyping.\n",
    "Once everything works, we'll remove this sampling step and train on the full dataset.\n",
    "\n",
    "**Important:** When sampling students, we need to also filter the VLE and assessment data\n",
    "to ONLY include rows for those sampled students. Otherwise, we'd have orphaned records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sample size for development\n",
    "# For proof-of-concept: Use 5,000 students (processes in ~30 minutes)\n",
    "# For final model: Set SAMPLE_SIZE = None to use all 32,593 students (processes in ~4 hours)\n",
    "SAMPLE_SIZE = 5000\n",
    "\n",
    "# Randomly sample students from the student_info table\n",
    "# Why student_info? It ensures one row per student-course enrollment\n",
    "# random_state=42 ensures reproducibility - same sample every time we run this\n",
    "student_sample = student_info.sample(\n",
    "    n=min(SAMPLE_SIZE, len(student_info)),  # Don't sample more than available\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Extract the IDs of sampled students\n",
    "# .unique() ensures no duplicates (though shouldn't be any in student_info)\n",
    "student_ids = student_sample['id_student'].unique()\n",
    "\n",
    "# Filter VLE data to ONLY include clicks from sampled students\n",
    "# .isin() checks if each row's id_student is in our sample\n",
    "vle_sample = student_vle[student_vle['id_student'].isin(student_ids)]\n",
    "\n",
    "# Filter assessment submissions to ONLY include sampled students\n",
    "assess_sample = student_assessment[student_assessment['id_student'].isin(student_ids)]\n",
    "\n",
    "print(f\"Sampled {len(student_sample)} students\")\n",
    "print(f\"VLE rows for sample: {len(vle_sample):,}\")  # Expect ~1-2 million\n",
    "print(f\"Assessment rows for sample: {len(assess_sample):,}\")  # Expect ~25k\n",
    "\n",
    "# Memory optimization: The original dataframes are still in memory\n",
    "# For large datasets, you might want to delete them:\n",
    "# del student_vle, student_assessment\n",
    "# import gc; gc.collect()  # Force garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "**The heart of the project:** Converting raw clickstream data into meaningful procrastination indicators.\n",
    "\n",
    "**The challenge:** We have millions of rows saying \"Student X clicked Resource Y on Day Z\".\n",
    "We need to transform this into features like \"Student X submits 75% of assignments late and studies irregularly.\"\n",
    "\n",
    "**Our approach:** For each student, we'll calculate:\n",
    "1. **Late submission patterns:** How often they miss deadlines\n",
    "2. **Study irregularity:** How inconsistent their daily engagement is\n",
    "3. **Last-minute behavior:** Do they cram right before deadlines?\n",
    "4. **Study gaps:** Long periods without any activity\n",
    "5. **Overall engagement:** Total clicks and active days\n",
    "\n",
    "**Why these features?**\n",
    "Research shows procrastinators exhibit these specific behaviors:\n",
    "- They know deadlines but wait until the last minute\n",
    "- Their study patterns are erratic (all-nighters vs. days off)\n",
    "- They have longer gaps between study sessions\n",
    "- They engage less overall compared to non-procrastinators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main feature engineering function\n",
    "# This function takes raw OULAD tables and outputs one row per student with computed features\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def engineer_features(vle_data, assess_data, assess_info, student_data):\n",
    "    \"\"\"\n",
    "    Engineer behavioral features from OULAD raw data to predict procrastination risk.\n",
    "    \n",
    "    Args:\n",
    "        vle_data: DataFrame of student VLE clicks (studentVle.csv)\n",
    "        assess_data: DataFrame of assessment submissions (studentAssessment.csv)\n",
    "        assess_info: DataFrame of assessment metadata with deadlines (assessments.csv)\n",
    "        student_data: DataFrame of student info to identify unique students (studentInfo.csv)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with one row per student and the following features:\n",
    "        - late_rate: Proportion of assignments submitted late (0 to 1)\n",
    "        - avg_days_late: Average number of days late across all submissions (can be negative if early)\n",
    "        - irregularity: Coefficient of variation of daily clicks (std/mean) - higher = more irregular\n",
    "        - last_min_ratio: Proportion of total clicks in the week before deadlines\n",
    "        - avg_gap: Average days between study sessions\n",
    "        - max_gap: Longest gap between study sessions (in days)\n",
    "        - total_clicks: Total clicks across entire course\n",
    "        - active_days: Number of distinct days with any activity\n",
    "        - num_assessments: How many assignments this student submitted\n",
    "    \"\"\"\n",
    "    features = []  # Will store dictionaries, one per student\n",
    "\n",
    "    # Get unique student-course combinations\n",
    "    # A student taking multiple courses should have separate feature rows\n",
    "    # Why drop_duplicates? In case student_data has duplicate rows\n",
    "    unique_students = student_data[\n",
    "        ['code_module', 'code_presentation', 'id_student']\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    # Process each student individually\n",
    "    # This is slow (O(n) for n students), but necessary since each student's features\n",
    "    # depend on their unique behavioral patterns\n",
    "    for idx, (module, presentation, sid) in enumerate(unique_students.values):\n",
    "        # Progress indicator - print every 1000 students so we know it's not stuck\n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"Processing {idx}/{len(unique_students)}...\")\n",
    "\n",
    "        # ===== STEP 1: EXTRACT THIS STUDENT'S VLE ACTIVITY =====\n",
    "        # Filter VLE data to this specific student in this specific course\n",
    "        # We need all three conditions because:\n",
    "        #   - Students can take multiple courses (need code_module)\n",
    "        #   - Same course can run multiple semesters (need code_presentation)\n",
    "        #   - Multiple students take the same course (need id_student)\n",
    "        s_vle = vle_data[\n",
    "            (vle_data['code_module'] == module) &\n",
    "            (vle_data['code_presentation'] == presentation) &\n",
    "            (vle_data['id_student'] == sid)\n",
    "        ]\n",
    "\n",
    "        # ===== STEP 2: EXTRACT THIS STUDENT'S ASSESSMENT SUBMISSIONS =====\n",
    "        # Get all submissions by this student (across all their courses)\n",
    "        s_assess = assess_data[\n",
    "            assess_data['id_student'] == sid\n",
    "        ]\n",
    "\n",
    "        # JOIN with assessment metadata to get deadlines\n",
    "        # This adds a 'date' column (the deadline) to each submission\n",
    "        # how='left' means keep all submissions even if deadline is missing\n",
    "        s_assess_full = s_assess.merge(\n",
    "            assess_info[['id_assessment', 'date']],\n",
    "            on='id_assessment',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # ===== FEATURE 1 & 2: LATE SUBMISSION PATTERNS =====\n",
    "        if len(s_assess_full) > 0:\n",
    "            # Calculate how many days late each submission was\n",
    "            # Positive value = submitted late\n",
    "            # Negative value = submitted early\n",
    "            # Zero = submitted exactly on deadline\n",
    "            s_assess_full['days_late'] = (\n",
    "                s_assess_full['date_submitted'] - s_assess_full['date']\n",
    "            )\n",
    "\n",
    "            # late_rate: What proportion of submissions were late?\n",
    "            # Example: If 3 out of 4 assignments were late, late_rate = 0.75\n",
    "            late_rate = (s_assess_full['days_late'] > 0).mean()\n",
    "\n",
    "            # avg_late: On average, how many days late?\n",
    "            # Example: Submitted 2 days late, 1 day late, 5 days early → avg = (2+1-5)/3 = -0.67\n",
    "            # Negative avg_late means student typically submits early (good!)\n",
    "            avg_late = s_assess_full['days_late'].mean()\n",
    "        else:\n",
    "            # Student never submitted any assessments\n",
    "            # This is rare but possible (complete disengagement or immediate dropout)\n",
    "            late_rate = 0\n",
    "            avg_late = 0\n",
    "\n",
    "        # ===== FEATURES 3-6: VLE ENGAGEMENT PATTERNS =====\n",
    "        if len(s_vle) > 0:\n",
    "            # Aggregate clicks per day\n",
    "            # A student might have multiple rows per day (clicked different resources)\n",
    "            # We sum them to get total clicks per day\n",
    "            daily = s_vle.groupby('date')['sum_click'].sum()\n",
    "\n",
    "            # FEATURE 3: irregularity - Coefficient of Variation (CV) of daily clicks\n",
    "            # CV = standard_deviation / mean\n",
    "            # Why CV instead of just std?\n",
    "            #   - If Student A clicks 100±20 per day and Student B clicks 10±2 per day,\n",
    "            #     both have similar relative variability even though std differs\n",
    "            # Interpretation:\n",
    "            #   - CV ~ 0.5: Consistent (e.g., 20, 25, 18, 22 clicks per day)\n",
    "            #   - CV ~ 2.0: Highly irregular (e.g., 0, 0, 100, 5, 0, 200)\n",
    "            #   - High CV = procrastination signal (bursty, inconsistent engagement)\n",
    "            irregularity = daily.std() / daily.mean() if daily.mean() > 0 else 0\n",
    "\n",
    "            # FEATURE 4: total_clicks - Overall engagement level\n",
    "            # Simple sum of all clicks across the semester\n",
    "            # Typically 500-5000 for active students, <500 for disengaged\n",
    "            total_clicks = daily.sum()\n",
    "\n",
    "            # FEATURE 5: active_days - How many distinct days had any activity\n",
    "            # Example: If course is 200 days and student was active 100 days → 50% attendance\n",
    "            # Procrastinators tend to have lower active_days (longer gaps)\n",
    "            active_days = len(daily)\n",
    "\n",
    "            # FEATURES 6 & 7: Study gaps - Time between sessions\n",
    "            # Get sorted list of unique dates when student was active\n",
    "            dates = np.array(sorted(s_vle['date'].unique()))\n",
    "\n",
    "            if len(dates) > 1:\n",
    "                # Calculate gaps between consecutive study days\n",
    "                # np.diff([1, 3, 10]) = [2, 7] (gaps of 2 days and 7 days)\n",
    "                gaps = np.diff(dates)\n",
    "\n",
    "                # avg_gap: Average days between study sessions\n",
    "                # Consistent students: ~1-2 days\n",
    "                # Procrastinators: 5-10 days (long periods of inactivity)\n",
    "                avg_gap = gaps.mean()\n",
    "\n",
    "                # max_gap: Longest period without any activity\n",
    "                # If max_gap > 14 days, student took a 2-week break (red flag)\n",
    "                max_gap = gaps.max()\n",
    "            else:\n",
    "                # Student only logged in one day (extreme disengagement)\n",
    "                avg_gap = 0\n",
    "                max_gap = 0\n",
    "        else:\n",
    "            # Student never clicked anything (immediate dropout or data error)\n",
    "            irregularity = 0\n",
    "            total_clicks = 0\n",
    "            active_days = 0\n",
    "            avg_gap = 0\n",
    "            max_gap = 0\n",
    "\n",
    "        # ===== FEATURE 8: LAST-MINUTE BEHAVIOR =====\n",
    "        # Do they cram in the week before deadlines?\n",
    "        last_min_ratio = 0\n",
    "        if len(s_vle) > 0 and len(s_assess_full) > 0:\n",
    "            # For each assignment deadline...\n",
    "            for deadline in s_assess_full['date'].dropna():  # dropna ignores missing deadlines\n",
    "                # Count clicks in the 7 days before deadline\n",
    "                # Why 7 days? Research shows procrastinators often start exactly one week before\n",
    "                week_clicks = s_vle[\n",
    "                    (s_vle['date'] >= deadline - 7) &  # Starting 7 days before\n",
    "                    (s_vle['date'] <= deadline)  # Up to and including deadline day\n",
    "                ]['sum_click'].sum()\n",
    "\n",
    "                # Accumulate clicks across all deadlines\n",
    "                last_min_ratio += week_clicks\n",
    "\n",
    "            # Convert to proportion: What % of ALL clicks happened near deadlines?\n",
    "            # Example: If student clicked 1000 times total, and 600 were in deadline weeks\n",
    "            #          → last_min_ratio = 0.6 (60% of effort was last-minute)\n",
    "            # Interpretation:\n",
    "            #   - last_min_ratio < 0.3: Spread out work consistently\n",
    "            #   - last_min_ratio > 0.6: Deadline-driven procrastinator\n",
    "            last_min_ratio = (\n",
    "                last_min_ratio / total_clicks if total_clicks > 0 else 0\n",
    "            )\n",
    "\n",
    "        # ===== COLLECT ALL FEATURES FOR THIS STUDENT =====\n",
    "        features.append({\n",
    "            'id_student': sid,\n",
    "            'code_module': module,\n",
    "            'code_presentation': presentation,\n",
    "            'late_rate': late_rate,\n",
    "            'avg_days_late': avg_late,\n",
    "            'irregularity': irregularity,\n",
    "            'last_min_ratio': last_min_ratio,\n",
    "            'avg_gap': avg_gap,\n",
    "            'max_gap': max_gap,\n",
    "            'total_clicks': total_clicks,\n",
    "            'active_days': active_days,\n",
    "            'num_assessments': len(s_assess_full)\n",
    "        })\n",
    "\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    return pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run feature engineering on our sampled data\n",
    "# This will take 5-30 minutes depending on sample size\n",
    "\n",
    "print(\"Engineering features...\")\n",
    "features_df = engineer_features(vle_sample, assess_sample, assessments, student_sample)\n",
    "print(f\"Created {len(features_df)} feature vectors\")\n",
    "\n",
    "# Expected output: 5000 rows (one per student in sample)\n",
    "# If you see fewer, some students might have been filtered out due to missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the engineered features\n",
    "features_df.head()\n",
    "\n",
    "# Sanity checks to perform on this output:\n",
    "# 1. late_rate should be between 0 and 1\n",
    "# 2. avg_days_late can be negative (if student submits early on average)\n",
    "# 3. irregularity is typically 0.5 to 3.0 (higher = more irregular)\n",
    "# 4. last_min_ratio should be between 0 and 1\n",
    "# 5. total_clicks varies widely (100 to 10,000+)\n",
    "# 6. If you see many zeros, might indicate data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataset by removing invalid students\n",
    "# We need students who have:\n",
    "#   1. At least one assessment (otherwise late_rate is meaningless)\n",
    "#   2. At least some clicks (otherwise irregularity is undefined)\n",
    "\n",
    "features_clean = features_df[\n",
    "    (features_df['num_assessments'] > 0) &  # Has submitted at least one assignment\n",
    "    (features_df['total_clicks'] > 0)  # Has clicked something\n",
    "].copy()  # .copy() prevents SettingWithCopyWarning when modifying later\n",
    "\n",
    "print(f\"Valid students after filtering: {len(features_clean)}\")\n",
    "# Typically lose 5-10% of students to this filter\n",
    "# Lost students are usually:\n",
    "#   - Immediate dropouts (never engaged)\n",
    "#   - Enrolled but never started\n",
    "#   - Data quality issues (missing records)\n",
    "\n",
    "# Why is this okay?\n",
    "# For procrastination prediction, we NEED behavioral data to make predictions\n",
    "# If a student never clicked or never submitted anything, we can't assess their procrastination\n",
    "# In production, these students would be flagged as \"Insufficient data\" rather than predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Distributions\n",
    "\n",
    "**Before building models, we need to understand our features:**\n",
    "- Are they normally distributed or skewed?\n",
    "- Do they have outliers?\n",
    "- What's the typical range?\n",
    "\n",
    "This informs decisions about:\n",
    "- Feature scaling (do we need StandardScaler?)\n",
    "- Outlier handling (cap extreme values?)\n",
    "- Feature transformation (log transform skewed features?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions of procrastination features\n",
    "features_to_plot = ['late_rate', 'avg_days_late', 'irregularity', 'last_min_ratio', 'avg_gap', 'max_gap']\n",
    "\n",
    "# Create a 2x3 grid of histograms\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()  # Convert 2D array to 1D for easier iteration\n",
    "\n",
    "for idx, feat in enumerate(features_to_plot):\n",
    "    # Plot histogram with 40 bins for granularity\n",
    "    axes[idx].hist(\n",
    "        features_clean[feat].fillna(0),  # Fill NaN with 0 for plotting\n",
    "        bins=40,\n",
    "        edgecolor='black',  # Black borders make bars more visible\n",
    "        alpha=0.7  # Slight transparency\n",
    "    )\n",
    "    axes[idx].set_title(feat.replace('_', ' ').title())  # Pretty title\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()  # Prevent overlapping labels\n",
    "plt.show()\n",
    "\n",
    "# What to look for:\n",
    "# - late_rate: Often bimodal (peak at 0 = always on time, peak at 1 = always late)\n",
    "# - irregularity: Right-skewed (most students ~1.0, some extreme at 5+)\n",
    "# - last_min_ratio: Should span 0 to 1, often normal-ish distribution\n",
    "# - avg_gap: Right-skewed (most 1-3 days, long tail to 20+)\n",
    "# - max_gap: Very right-skewed (some students have 30+ day breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistical summary of features\n",
    "features_clean[features_to_plot].describe()\n",
    "\n",
    "# Key statistics to check:\n",
    "# - mean: Center of distribution\n",
    "# - std: Spread (high std = high variance, might need scaling)\n",
    "# - min/max: Check for impossible values (late_rate outside 0-1?)\n",
    "# - 25%/50%/75%: Quartiles help identify skewness\n",
    "#\n",
    "# Example interpretation:\n",
    "# If irregularity has mean=1.5, std=1.2, this means:\n",
    "#   - Typical student has CV around 1.5 (moderately irregular)\n",
    "#   - One std above (2.7) is quite irregular\n",
    "#   - Two std above (3.9) is extreme procrastination pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis\n",
    "\n",
    "**Why check correlations?**\n",
    "- **Multicollinearity:** If two features are highly correlated (r > 0.8), they're redundant\n",
    "  - Example: If `late_rate` and `avg_days_late` have r=0.95, they measure the same thing\n",
    "  - Solution: Drop one to reduce model complexity\n",
    "- **Feature relationships:** Do features measure related aspects of procrastination?\n",
    "  - We WANT moderate correlations (0.3-0.6) between our procrastination indicators\n",
    "  - This suggests they capture different but related behaviors\n",
    "- **Independence:** Features with r~0 measure completely different things\n",
    "  - Good for model diversity, but might indicate feature isn't relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for procrastination features\n",
    "corr_matrix = features_clean[features_to_plot].corr()\n",
    "\n",
    "# Visualize as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,  # Show correlation values in cells\n",
    "    fmt='.2f',  # Format to 2 decimal places\n",
    "    cmap='coolwarm',  # Red=positive correlation, Blue=negative\n",
    "    center=0,  # White at zero correlation\n",
    "    square=True,  # Make cells square\n",
    "    linewidths=1  # Add gridlines\n",
    ")\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# How to read this:\n",
    "# - Diagonal is always 1.0 (feature perfectly correlates with itself)\n",
    "# - Off-diagonal shows pairwise correlations\n",
    "# - Values range from -1 (perfect negative) to +1 (perfect positive)\n",
    "#\n",
    "# What we expect to see:\n",
    "# - late_rate ↔ avg_days_late: HIGH positive (0.4-0.6)\n",
    "#   Students who submit late often are late by more days on average\n",
    "# - irregularity ↔ last_min_ratio: MODERATE positive (0.2-0.4)\n",
    "#   Irregular students often cram before deadlines\n",
    "# - avg_gap ↔ max_gap: HIGH positive (0.6-0.8)\n",
    "#   Students with long average gaps also have long maximum gaps\n",
    "#\n",
    "# Red flags:\n",
    "# - Any correlation > 0.9: Consider dropping one feature\n",
    "# - All correlations < 0.1: Features might not be measuring procrastination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "**THE MOST IMPORTANT SECTION FOR YOUR PROJECT**\n",
    "\n",
    "**The Problem:** We don't have ground truth labels saying \"Student X is a procrastinator.\"\n",
    "OULAD doesn't include procrastination labels. We only have behavioral features.\n",
    "\n",
    "**The Solution:** Use K-Means clustering to automatically group students into risk categories\n",
    "based on their behavioral patterns.\n",
    "\n",
    "**How K-Means works:**\n",
    "1. Choose k (number of clusters) - we'll use k=3 for Low/Medium/High risk\n",
    "2. Algorithm finds k \"centroids\" (cluster centers) that minimize within-cluster variance\n",
    "3. Each student is assigned to the nearest centroid\n",
    "4. We examine cluster characteristics to label them as Low/Medium/High risk\n",
    "\n",
    "**Why this is valid for procrastination:**\n",
    "- Procrastination is a spectrum, not binary (everyone procrastinates sometimes)\n",
    "- Students naturally cluster into distinct behavioral patterns\n",
    "- By using multiple features (late_rate, irregularity, last_min_ratio, avg_gap),\n",
    "  we capture different dimensions of procrastination behavior\n",
    "- Research validates that these features correlate with procrastination\n",
    "\n",
    "**The output:** Each student gets a cluster label (0, 1, or 2) which we'll map to risk levels.\n",
    "This becomes our **target variable** for supervised learning (the Bi-LSTM model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which features to use for clustering\n",
    "# We're using 4 key procrastination indicators:\n",
    "cluster_features = ['late_rate', 'irregularity', 'last_min_ratio', 'avg_gap']\n",
    "\n",
    "# Why these 4?\n",
    "# - late_rate: Direct measure of missing deadlines\n",
    "# - irregularity: Captures inconsistent study patterns\n",
    "# - last_min_ratio: Identifies deadline-driven behavior\n",
    "# - avg_gap: Measures study consistency\n",
    "#\n",
    "# Why NOT include:\n",
    "# - max_gap: Highly correlated with avg_gap (redundant)\n",
    "# - total_clicks: Measures engagement level, not procrastination pattern\n",
    "# - active_days: Similar issue to total_clicks\n",
    "# - avg_days_late: Highly correlated with late_rate\n",
    "\n",
    "# Extract clustering features and fill missing values with 0\n",
    "# .fillna(0) is reasonable here because:\n",
    "#   - Missing late_rate means no late submissions (= 0)\n",
    "#   - Missing irregularity means no variation to calculate (= 0)\n",
    "X_cluster = features_clean[cluster_features].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features to have mean=0 and std=1\n",
    "# This is CRITICAL for K-Means because:\n",
    "#   - K-Means uses Euclidean distance to assign students to clusters\n",
    "#   - If late_rate is 0-1 and avg_gap is 0-30, avg_gap dominates the distance calculation\n",
    "#   - Scaling makes all features contribute equally\n",
    "#\n",
    "# StandardScaler formula: z = (x - mean) / std\n",
    "# Example: If irregularity has mean=1.5, std=1.0\n",
    "#   - Value 1.5 becomes 0 (at the mean)\n",
    "#   - Value 2.5 becomes 1 (one std above mean)\n",
    "#   - Value 0.5 becomes -1 (one std below mean)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_cluster)\n",
    "\n",
    "# IMPORTANT: Save this scaler!\n",
    "# When we deploy the model, we need to scale new student data using the SAME\n",
    "# mean and std from training. Otherwise predictions will be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal number of clusters (k)\n",
    "# We'll try k from 2 to 7 and use two metrics to decide:\n",
    "\n",
    "inertias = []  # Within-cluster sum of squares (want lower)\n",
    "silhouettes = []  # Silhouette score (want higher)\n",
    "K_range = range(2, 8)  # Try 2, 3, 4, 5, 6, 7 clusters\n",
    "\n",
    "for k in K_range:\n",
    "    # Fit K-Means with k clusters\n",
    "    # random_state=42: Ensures reproducibility (same results each run)\n",
    "    # n_init=10: Run algorithm 10 times with different initializations, keep best\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Metric 1: Inertia (within-cluster sum of squared distances)\n",
    "    # - Measures how tight clusters are\n",
    "    # - Always decreases as k increases (more clusters = tighter fit)\n",
    "    # - Look for \"elbow\" where adding more clusters doesn't help much\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Metric 2: Silhouette score (-1 to 1)\n",
    "    # - Measures how well-separated clusters are\n",
    "    # - Higher is better (1 = perfect separation)\n",
    "    # - Accounts for both cluster tightness AND separation\n",
    "    # - More reliable than inertia for choosing k\n",
    "    silhouettes.append(silhouette_score(X_scaled, labels))\n",
    "\n",
    "# Note: This takes a few minutes because we're clustering 5000 students 6 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize elbow method and silhouette analysis\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Elbow method (inertia vs k)\n",
    "ax1.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax1.set_ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)\n",
    "ax1.set_title('Elbow Method for Optimal k', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "# Look for the \"elbow\" - where curve starts to flatten\n",
    "# Sharp drop from k=2 to k=3, then flattens → elbow around k=3 or k=4\n",
    "\n",
    "# Plot 2: Silhouette analysis\n",
    "ax2.plot(K_range, silhouettes, 'ro-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "ax2.set_ylabel('Silhouette Score', fontsize=12)\n",
    "ax2.set_title('Silhouette Analysis for Optimal k', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "# Look for the maximum silhouette score\n",
    "# Higher score = better-defined clusters\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Decision process:\n",
    "# - Silhouette might peak at k=6 (best separation)\n",
    "# - But elbow is at k=3 (diminishing returns after)\n",
    "# - For interpretability, we want 3 clusters (Low/Medium/High risk)\n",
    "# - k=6 would require labeling 6 risk levels, which is too granular for stakeholders\n",
    "# → Choose k=3 as a balance between statistical quality and practical use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit K-Means with optimal k=3\n",
    "optimal_k = 3\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "\n",
    "# Assign cluster labels to each student\n",
    "# Clusters are numbered 0, 1, 2 (arbitrary labels, not ordered)\n",
    "features_clean['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Check cluster sizes\n",
    "print(\"Students per cluster:\")\n",
    "print(features_clean['cluster'].value_counts().sort_index())\n",
    "\n",
    "# What to look for:\n",
    "# - Balanced clusters (each has 20-40% of students) → good\n",
    "# - One cluster has 90% of students → bad (not meaningful separation)\n",
    "# - One cluster has <5% → might be outliers, not a true pattern\n",
    "#\n",
    "# Typical OULAD distribution:\n",
    "# - Cluster 0: ~50% (Low risk - consistent, on-time students)\n",
    "# - Cluster 1: ~35% (Medium risk - occasional late submissions, some irregularity)\n",
    "# - Cluster 2: ~15% (High risk - frequent late work, very irregular patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine cluster characteristics to understand what each cluster represents\n",
    "# This is how we determine which cluster is Low/Medium/High risk\n",
    "\n",
    "cluster_summary = features_clean.groupby('cluster')[cluster_features].mean()\n",
    "print(\"\\nAverage feature values per cluster:\")\n",
    "print(cluster_summary)\n",
    "\n",
    "# How to interpret this table:\n",
    "# Look at each cluster's average values for the 4 features\n",
    "#\n",
    "# Example interpretation:\n",
    "# Cluster 0: late_rate=0.15, irregularity=0.8, last_min_ratio=0.25, avg_gap=2.1\n",
    "#   → Low risk (rarely late, consistent study, not deadline-driven, short gaps)\n",
    "#\n",
    "# Cluster 1: late_rate=0.45, irregularity=1.5, last_min_ratio=0.45, avg_gap=4.5\n",
    "#   → Medium risk (sometimes late, moderately irregular, some cramming, longer gaps)\n",
    "#\n",
    "# Cluster 2: late_rate=0.85, irregularity=2.8, last_min_ratio=0.70, avg_gap=8.2\n",
    "#   → High risk (usually late, very irregular, heavy cramming, long gaps)\n",
    "#\n",
    "# The cluster with the HIGHEST average across procrastination indicators = High Risk\n",
    "# The cluster with the LOWEST average = Low Risk\n",
    "# Middle cluster = Medium Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically assign risk labels based on cluster characteristics\n",
    "# We'll use a composite risk score combining the 3 main procrastination indicators\n",
    "\n",
    "# Calculate average risk score for each cluster\n",
    "# We exclude avg_gap because it's measured in days (different scale from 0-1 features)\n",
    "risk_scores = cluster_summary[['late_rate', 'irregularity', 'last_min_ratio']].mean(axis=1)\n",
    "\n",
    "# Sort clusters by risk score (ascending)\n",
    "# cluster_order[0] = cluster with lowest risk score (Low Risk)\n",
    "# cluster_order[1] = cluster with medium risk score (Medium Risk)\n",
    "# cluster_order[2] = cluster with highest risk score (High Risk)\n",
    "cluster_order = risk_scores.sort_values().index.tolist()\n",
    "\n",
    "# Create mapping from cluster number to risk label\n",
    "risk_map = {\n",
    "    cluster_order[0]: 'Low',\n",
    "    cluster_order[1]: 'Medium',\n",
    "    cluster_order[2]: 'High'\n",
    "}\n",
    "\n",
    "# Apply risk labels to all students\n",
    "features_clean['risk'] = features_clean['cluster'].map(risk_map)\n",
    "\n",
    "print(\"\\nRisk level distribution:\")\n",
    "print(features_clean['risk'].value_counts())\n",
    "\n",
    "# Sanity check: Does the distribution make sense?\n",
    "# We expect:\n",
    "# - Low risk: Plurality (40-50% of students)\n",
    "# - Medium risk: Significant minority (30-40%)\n",
    "# - High risk: Smaller group (10-20%)\n",
    "#\n",
    "# If High risk is 50%+, something went wrong (too pessimistic)\n",
    "# If High risk is <5%, model might not be sensitive enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Visualization\n",
    "\n",
    "**Challenge:** We clustered on 4 features, but we can only visualize in 2D or 3D.\n",
    "\n",
    "**Solution:** Use PCA (Principal Component Analysis) to project 4D data into 2D while\n",
    "preserving as much variance as possible.\n",
    "\n",
    "**What PCA does:**\n",
    "- Finds the two directions (principal components) that explain the most variance\n",
    "- PC1 (x-axis): Direction of maximum variance in the data\n",
    "- PC2 (y-axis): Direction of second-most variance, perpendicular to PC1\n",
    "- Together, PC1 and PC2 might explain 60-80% of total variance\n",
    "\n",
    "**Interpretation:**\n",
    "- Well-separated clusters in PCA plot → clustering captured meaningful patterns\n",
    "- Overlapping clusters → boundaries are fuzzy (some students are borderline)\n",
    "- This is EXPECTED for procrastination (it's a spectrum, not discrete categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce 4D feature space to 2D for visualization\n",
    "pca = PCA(n_components=2)  # Keep only first 2 principal components\n",
    "X_pca = pca.fit_transform(X_scaled)  # Transform scaled data\n",
    "\n",
    "# X_pca is now a (n_students, 2) array where:\n",
    "# - Column 0 = PC1 (x-coordinate in 2D projection)\n",
    "# - Column 1 = PC2 (y-coordinate in 2D projection)\n",
    "\n",
    "# Create side-by-side visualizations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# ===== Plot 1: PCA Scatter Plot with Risk Labels =====\n",
    "# Color each point by its risk level\n",
    "for cluster in features_clean['cluster'].unique():\n",
    "    # Get students in this cluster\n",
    "    mask = features_clean['cluster'] == cluster\n",
    "    \n",
    "    # Get risk label for this cluster\n",
    "    risk = risk_map[cluster]\n",
    "    \n",
    "    # Plot this cluster's students\n",
    "    ax1.scatter(\n",
    "        X_pca[mask, 0],  # X-coordinates (PC1)\n",
    "        X_pca[mask, 1],  # Y-coordinates (PC2)\n",
    "        label=f'{risk} Risk',\n",
    "        alpha=0.6,  # Transparency to see overlapping points\n",
    "        s=30  # Point size\n",
    "    )\n",
    "\n",
    "# Add labels and legend\n",
    "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
    "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
    "ax1.set_title('Student Clusters Projected onto Principal Components', fontsize=14)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Interpretation guide:\n",
    "# - PC1 often represents \"overall procrastination severity\"\n",
    "#   (combination of late_rate, irregularity, etc.)\n",
    "# - PC2 often represents a specific pattern (e.g., deadline-driven vs. consistently low effort)\n",
    "# - If explained_variance_ratio[0] = 45%, PC1 captures 45% of the variance in the data\n",
    "# - If both PCs together explain >60%, the 2D plot is a good representation\n",
    "\n",
    "# ===== Plot 2: Risk Distribution Bar Chart =====\n",
    "risk_counts = features_clean['risk'].value_counts()\n",
    "\n",
    "# Define colors for each risk level\n",
    "colors = {\n",
    "    'Low': '#2ecc71',  # Green\n",
    "    'Medium': '#f39c12',  # Orange\n",
    "    'High': '#e74c3c'  # Red\n",
    "}\n",
    "\n",
    "# Create bar chart\n",
    "ax2.bar(\n",
    "    risk_counts.index,\n",
    "    risk_counts.values,\n",
    "    color=[colors[x] for x in risk_counts.index]\n",
    ")\n",
    "ax2.set_xlabel('Risk Level', fontsize=12)\n",
    "ax2.set_ylabel('Number of Students', fontsize=12)\n",
    "ax2.set_title('Distribution of Procrastination Risk Levels', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add count labels on top of bars\n",
    "for i, (level, count) in enumerate(risk_counts.items()):\n",
    "    ax2.text(i, count, f'{count}\\n({count/len(features_clean):.1%})',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# What good clustering looks like:\n",
    "# - In Plot 1: Three visible groups (even if some overlap)\n",
    "# - Low risk cluster towards bottom-left (lower PC1 and PC2)\n",
    "# - High risk cluster towards top-right (higher PC1 and PC2)\n",
    "# - Medium risk in between or spreading across\n",
    "#\n",
    "# In Plot 2:\n",
    "# - Reasonable distribution (not 90% in one category)\n",
    "# - Low risk as plurality (40-50%)\n",
    "# - High risk as meaningful minority (10-20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis\n",
    "\n",
    "**Why we need this:**\n",
    "Before building an LSTM model, we need to verify that student behavior actually has\n",
    "**temporal structure** - patterns that evolve over time and have predictable sequences.\n",
    "\n",
    "**Questions we're answering:**\n",
    "1. Is daily click data **stationary**? (Does it have stable statistical properties?)\n",
    "2. What is the **autocorrelation structure**? (Do past days predict future days?)\n",
    "3. How far back should we look? (What's the optimal **sequence length** for LSTM?)\n",
    "\n",
    "**Why LSTM over traditional ML:**\n",
    "- Traditional ML (Random Forest, SVM) treats each student as independent features\n",
    "- LSTM treats each student as a SEQUENCE of daily behaviors\n",
    "- Example: RF sees [total_clicks=5000], LSTM sees [50, 0, 0, 200, 0, 100, ...]\n",
    "- LSTM can detect patterns like \"gradual disengagement\" or \"sudden re-engagement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample student to analyze\n",
    "# We'll use the first student in our features_clean dataset\n",
    "sample_student = features_clean.iloc[0]\n",
    "\n",
    "# Extract this student's click history\n",
    "student_clicks = vle_sample[\n",
    "    (vle_sample['id_student'] == sample_student['id_student']) &\n",
    "    (vle_sample['code_module'] == sample_student['code_module']) &\n",
    "    (vle_sample['code_presentation'] == sample_student['code_presentation'])\n",
    "]\n",
    "\n",
    "# Aggregate clicks per day and sort chronologically\n",
    "# This gives us a time series: [day_0: 50 clicks, day_1: 30 clicks, ...]\n",
    "daily_clicks = student_clicks.groupby('date')['sum_click'].sum().sort_index()\n",
    "\n",
    "print(f\"Analyzing student {sample_student['id_student']}\")\n",
    "print(f\"Course: {sample_student['code_module']}-{sample_student['code_presentation']}\")\n",
    "print(f\"Days of activity: {len(daily_clicks)}\")\n",
    "print(f\"Total clicks: {daily_clicks.sum()}\")\n",
    "print(f\"Risk level: {sample_student['risk']}\")\n",
    "\n",
    "# Note: We're analyzing ONE student to understand the time series properties\n",
    "# These properties (autocorrelation, stationarity) tend to be similar across students\n",
    "# If we wanted to be thorough, we'd average ACF/PACF across many students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize daily click pattern\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(daily_clicks.values, linewidth=1.5, color='steelblue')\n",
    "plt.title(f'Daily Click Pattern - Student {sample_student[\"id_student\"]} ({sample_student[\"risk\"]} Risk)',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Day of Course', fontsize=12)\n",
    "plt.ylabel('Total Clicks', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# What to look for in this plot:\n",
    "# - Overall trend: Increasing (growing engagement), decreasing (burnout), or stable?\n",
    "# - Spikes: Do they correspond to assignment deadlines? (indicates last-minute behavior)\n",
    "# - Gaps: Long periods of zero clicks? (procrastination indicator)\n",
    "# - Pattern regularity: \n",
    "#   * Regular pattern (consistent peaks/valleys) → good self-regulation\n",
    "#   * Chaotic pattern (erratic spikes) → poor self-regulation\n",
    "#\n",
    "# Example High Risk pattern:\n",
    "# [5, 0, 0, 0, 0, 0, 150, 10, 0, 0, 0, 0, 0, 200, ...]\n",
    "# (Long gaps followed by sudden spikes - classic procrastination)\n",
    "#\n",
    "# Example Low Risk pattern:\n",
    "# [25, 30, 28, 32, 35, 22, 30, 28, 31, 27, ...]\n",
    "# (Consistent daily engagement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for stationarity using Augmented Dickey-Fuller (ADF) test\n",
    "#\n",
    "# **What is stationarity?**\n",
    "# A time series is stationary if its statistical properties (mean, variance, autocorrelation)\n",
    "# don't change over time.\n",
    "#\n",
    "# **Why it matters for LSTM:**\n",
    "# - Non-stationary data has trends or seasonality that confuse the model\n",
    "# - Example: If clicks steadily increase from 10→100 over the semester due to\n",
    "#   accumulating materials, the LSTM might learn the trend instead of the pattern\n",
    "# - Stationary data: Patterns are consistent (what works for week 1 works for week 10)\n",
    "#\n",
    "# **ADF test:**\n",
    "# - Null hypothesis (H0): Time series has a unit root (non-stationary)\n",
    "# - Alternative hypothesis (H1): Time series is stationary\n",
    "# - If p-value < 0.05: Reject H0 → data IS stationary (good for LSTM)\n",
    "# - If p-value >= 0.05: Fail to reject H0 → data might have trends (need differencing)\n",
    "\n",
    "adf_result = adfuller(daily_clicks.values)\n",
    "\n",
    "print(\"\\n=== Stationarity Test (ADF) ===\")\n",
    "print(f\"ADF Statistic: {adf_result[0]:.4f}\")\n",
    "print(f\"p-value: {adf_result[1]:.4f}\")\n",
    "print(f\"Critical values: {adf_result[4]}\")\n",
    "\n",
    "if adf_result[1] < 0.05:\n",
    "    print(\"✓ Time series is STATIONARY (p < 0.05)\")\n",
    "    print(\"  → Good for LSTM modeling without transformation\")\n",
    "else:\n",
    "    print(\"✗ Time series is NON-STATIONARY (p >= 0.05)\")\n",
    "    print(\"  → May need differencing or detrending before modeling\")\n",
    "\n",
    "# Most OULAD students show stationary patterns because:\n",
    "# - No long-term trends (engagement doesn't consistently increase/decrease)\n",
    "# - Variance is relatively stable (spikes occur throughout, not just at end)\n",
    "# - If you see p > 0.05, the student might have unusual growth/decline pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ACF and PACF plots\n",
    "#\n",
    "# **ACF (Autocorrelation Function):**\n",
    "# - Shows correlation between the time series and lagged versions of itself\n",
    "# - ACF at lag k = correlation between [clicks on day t] and [clicks on day t-k]\n",
    "# - Example: ACF at lag 1 = \"How much does yesterday predict today?\"\n",
    "#            ACF at lag 7 = \"How much does last week predict today?\"\n",
    "#\n",
    "# **PACF (Partial Autocorrelation Function):**\n",
    "# - Like ACF but removes indirect correlations\n",
    "# - PACF at lag k = direct correlation, not through intermediate lags\n",
    "# - Example: If day t-2 influences day t-1, which influences day t,\n",
    "#            ACF would show correlation at both lag 1 and lag 2\n",
    "#            PACF would only show correlation at lag 1 (direct influence)\n",
    "#\n",
    "# **How to use these for LSTM:**\n",
    "# - If ACF shows significant correlation up to lag L, use sequence length >= L\n",
    "# - Example: If ACF is significant up to lag 14, use 14-day sequences for LSTM\n",
    "# - If ACF drops to zero after lag 7, using 30-day sequences adds noise\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Plot ACF\n",
    "plot_acf(daily_clicks.values, lags=30, ax=ax1, alpha=0.05)\n",
    "# lags=30: Show correlations up to 30 days back\n",
    "# alpha=0.05: Significance level (95% confidence intervals shown as blue shaded area)\n",
    "ax1.set_title('Autocorrelation Function (ACF)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Lag (days)', fontsize=12)\n",
    "ax1.set_ylabel('Autocorrelation', fontsize=12)\n",
    "\n",
    "# Plot PACF\n",
    "plot_pacf(daily_clicks.values, lags=30, ax=ax2, alpha=0.05, method='ywm')\n",
    "# method='ywm': Yule-Walker method (more stable for small samples than default)\n",
    "ax2.set_title('Partial Autocorrelation Function (PACF)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Lag (days)', fontsize=12)\n",
    "ax2.set_ylabel('Partial Autocorrelation', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# **How to read these plots:**\n",
    "#\n",
    "# Lag 0: Always 1.0 (series perfectly correlates with itself)\n",
    "# \n",
    "# Blue shaded area: Confidence interval\n",
    "# - Bars outside this area are statistically significant\n",
    "# - Bars inside are likely just random noise\n",
    "#\n",
    "# **Common patterns and interpretations:**\n",
    "#\n",
    "# Pattern 1: ACF decays slowly, PACF cuts off after lag 1\n",
    "#   → AR(1) process: Each day depends mainly on yesterday\n",
    "#   → LSTM sequence length: 3-7 days is sufficient\n",
    "#\n",
    "# Pattern 2: ACF has spike at lag 7, then decays\n",
    "#   → Weekly seasonality: Students study on similar weekdays\n",
    "#   → LSTM sequence length: Need at least 7 days, preferably 14-21 for multiple weeks\n",
    "#\n",
    "# Pattern 3: Both ACF and PACF cut off quickly (lag 2-3)\n",
    "#   → Short memory: Only recent days matter\n",
    "#   → LSTM sequence length: 3-7 days, longer provides no benefit\n",
    "#\n",
    "# **For your earlier ACF plots that showed lag-5 spike:**\n",
    "# This indicates weekly patterns (lag 5 ≈ weekdays, ignoring weekends)\n",
    "# → Recommended LSTM sequence length: 14-21 days to capture 2-3 weekly cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Generation for LSTM\n",
    "\n",
    "**The transformation:** Converting student behavioral data into sequences for LSTM training.\n",
    "\n",
    "**Current data structure:**\n",
    "- One row per student with aggregate features: `[student_123, late_rate=0.5, irregularity=2.1, ...]`\n",
    "\n",
    "**Target data structure:**\n",
    "- One 3D tensor: `(num_students, sequence_length, num_features)`\n",
    "- Example: `(5000 students, 30 days, 1 feature)` → shape (5000, 30, 1)\n",
    "- Each student is represented as a sequence of daily clicks: `[[day0], [day1], ..., [day29]]`\n",
    "\n",
    "**Why sequences?**\n",
    "- LSTMs are designed for sequential data\n",
    "- They learn patterns like \"3 days of low activity followed by a spike\" = procrastination\n",
    "- Or \"gradually decreasing engagement\" = burnout/dropout risk\n",
    "\n",
    "**Key decision: sequence_length**\n",
    "- Too short (3-7 days): Not enough context, might miss patterns\n",
    "- Too long (60+ days): Adds noise, overfits, slower training\n",
    "- Optimal (from your ACF): 14-30 days (based on lag-5 weekly pattern)\n",
    "- For your 1-week test: We'll also create 3-day and 7-day variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(vle_data, features_data, seq_len=30):\n",
    "    \"\"\"\n",
    "    Convert student VLE clickstream data into sequences for LSTM training.\n",
    "    \n",
    "    Args:\n",
    "        vle_data: DataFrame of VLE clicks (filtered to relevant students)\n",
    "        features_data: DataFrame with risk labels (from K-means clustering)\n",
    "        seq_len: Length of sequences (number of days to include)\n",
    "    \n",
    "    Returns:\n",
    "        X_sequences: 3D numpy array of shape (num_sequences, seq_len, 1)\n",
    "                     Contains normalized daily click counts\n",
    "        y_labels: 1D numpy array of shape (num_sequences,)\n",
    "                  Contains encoded risk labels (0=Low, 1=Medium, 2=High)\n",
    "        label_encoder: Fitted LabelEncoder to convert back to text labels\n",
    "    \n",
    "    Note: A single student can generate multiple sequences using a sliding window.\n",
    "    Example: If student has 100 days of data and seq_len=30,\n",
    "             we create 71 sequences: [days 0-29], [days 1-30], ..., [days 70-99]\n",
    "    \"\"\"\n",
    "    sequences = []  # Will store 2D arrays (seq_len, 1) for each sequence\n",
    "    labels = []  # Will store risk label for each sequence\n",
    "\n",
    "    # Encode text labels (\"Low\", \"Medium\", \"High\") to integers (0, 1, 2)\n",
    "    label_enc = LabelEncoder()\n",
    "    features_data['risk_enc'] = label_enc.fit_transform(features_data['risk'])\n",
    "    # label_enc.classes_ will be ['High', 'Low', 'Medium'] (alphabetical order)\n",
    "    # So High=0, Low=1, Medium=2\n",
    "    # This is fine - the neural network doesn't care about ordering\n",
    "\n",
    "    # Process each student individually\n",
    "    for idx, row in features_data.iterrows():\n",
    "        # Progress indicator\n",
    "        if idx % 500 == 0:\n",
    "            print(f\"Processing student {idx}/{len(features_data)}\")\n",
    "\n",
    "        # Extract this student's click history\n",
    "        student_data = vle_data[\n",
    "            (vle_data['id_student'] == row['id_student']) &\n",
    "            (vle_data['code_module'] == row['code_module']) &\n",
    "            (vle_data['code_presentation'] == row['code_presentation'])\n",
    "        ]\n",
    "\n",
    "        # Skip students with too little data\n",
    "        # If student only has 10 days of activity and seq_len=30, we can't create a full sequence\n",
    "        if len(student_data) < seq_len:\n",
    "            continue\n",
    "\n",
    "        # Aggregate clicks per day and sort chronologically\n",
    "        daily = student_data.groupby('date')['sum_click'].sum().sort_index()\n",
    "\n",
    "        # SLIDING WINDOW APPROACH:\n",
    "        # Create multiple sequences from each student's timeline\n",
    "        # This augments our dataset (more training examples) and helps LSTM learn\n",
    "        # temporal patterns at different points in the course\n",
    "        for i in range(len(daily) - seq_len):\n",
    "            # Extract sequence of length seq_len starting at day i\n",
    "            seq = daily.iloc[i:i+seq_len].values  # Shape: (seq_len,)\n",
    "\n",
    "            # NORMALIZATION (CRITICAL STEP):\n",
    "            # Divide by max to scale to [0, 1] range\n",
    "            # Why normalize per sequence (not globally)?\n",
    "            #   - Students have vastly different engagement levels\n",
    "            #   - Student A: 10-50 clicks/day, Student B: 200-500 clicks/day\n",
    "            #   - We care about PATTERNS (relative changes), not absolute numbers\n",
    "            #   - Normalizing preserves \"0 clicks = 0\", \"max clicks = 1\", \"typical day = 0.3-0.7\"\n",
    "            #   - Helps LSTM focus on behavioral patterns, not just engagement level\n",
    "            seq_norm = seq / seq.max() if seq.max() > 0 else seq  # Avoid division by zero\n",
    "\n",
    "            # Reshape to (seq_len, 1) for LSTM input\n",
    "            # The \"1\" means we have 1 feature per timestep (just click count)\n",
    "            # If we added multiple features (e.g., session duration), it would be (seq_len, 2)\n",
    "            sequences.append(seq_norm.reshape(-1, 1))\n",
    "\n",
    "            # All sequences from same student get the same label (their cluster assignment)\n",
    "            # This is a simplification - in reality, procrastination might change over semester\n",
    "            # But K-means labels represent overall tendency, which is what we're predicting\n",
    "            labels.append(row['risk_enc'])\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    # np.array(sequences) creates shape (num_sequences, seq_len, 1)\n",
    "    return np.array(sequences), np.array(labels), label_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sequences with 30-day windows\n",
    "# This will take 5-15 minutes depending on data size\n",
    "\n",
    "print(\"Creating 30-day sequences...\")\n",
    "X_seq, y_labels, label_enc = create_sequences(\n",
    "    vle_sample,\n",
    "    features_clean,\n",
    "    seq_len=30  # Use 30-day sequences (based on ACF showing significant correlation up to ~30 days)\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(X_seq)} sequences\")\n",
    "print(f\"Sequence shape: {X_seq.shape}\")  # Expected: (num_sequences, 30, 1)\n",
    "print(f\"Labels shape: {y_labels.shape}\")  # Expected: (num_sequences,)\n",
    "print(f\"\\nLabel encoding: {dict(enumerate(label_enc.classes_))}\")\n",
    "\n",
    "# Understanding the output:\n",
    "# If we have 5000 students, and each student has ~100 days of data on average,\n",
    "# and we create sequences with sliding windows:\n",
    "#   - Student with 100 days → (100-30) = 70 sequences\n",
    "#   - 5000 students × 70 sequences = 350,000 total sequences\n",
    "# This data augmentation dramatically increases training data!\n",
    "\n",
    "# Sanity check:\n",
    "# - X_seq should have shape (N, 30, 1) where N >> 5000 (due to sliding windows)\n",
    "# - y_labels should have same length as X_seq\n",
    "# - All values in X_seq should be between 0 and 1 (due to normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "**Why split?**\n",
    "- Training set (80%): Used to train the model (adjust weights, learn patterns)\n",
    "- Test set (20%): Held out to evaluate generalization (how well model predicts unseen students)\n",
    "\n",
    "**Important:** We evaluate on SEQUENCES, not students.\n",
    "- If Student A generated 70 sequences, some might be in train, some in test\n",
    "- This tests: \"Can the model recognize procrastination patterns at different time points?\"\n",
    "- Alternative approach (more strict): Split by STUDENTS (all of Student A's sequences in train or test)\n",
    "  - Pros: Tests generalization to completely new students\n",
    "  - Cons: Harder evaluation (requires more data for reliable test set)\n",
    "\n",
    "**Random state:** Setting random_state=42 ensures reproducibility.\n",
    "- Every time you run this cell, same sequences go to train vs test\n",
    "- Allows comparing different models fairly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sequences into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq,  # Sequences (input data)\n",
    "    y_labels,  # Risk labels (target)\n",
    "    test_size=0.2,  # 20% for testing, 80% for training\n",
    "    random_state=42,  # Reproducibility\n",
    "    stratify=y_labels  # Maintain class distribution in both sets\n",
    ")\n",
    "\n",
    "# stratify=y_labels ensures both train and test have similar proportions of Low/Med/High\n",
    "# Example: If overall data is 50% Low, 35% Med, 15% High,\n",
    "#          training set will also be ~50% Low, ~35% Med, ~15% High\n",
    "# Why? Prevents situation where test set has mostly High risk but train has mostly Low\n",
    "#      (would make evaluation misleading)\n",
    "\n",
    "print(\"Dataset split:\")\n",
    "print(f\"Training sequences: {len(X_train):,} ({len(X_train)/len(X_seq):.1%})\")\n",
    "print(f\"Testing sequences: {len(X_test):,} ({len(X_test)/len(X_seq):.1%})\")\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify class distribution in training set\n",
    "# This checks that stratification worked correctly\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "for cls, cnt in zip(label_enc.classes_, counts):\n",
    "    print(f\"{cls} risk: {cnt:,} sequences ({cnt/len(y_train):.1%})\")\n",
    "\n",
    "# Expected distribution (should match overall features_clean distribution):\n",
    "# Low: 45-55%\n",
    "# Medium: 30-40%\n",
    "# High: 10-20%\n",
    "\n",
    "# If distribution is severely imbalanced (e.g., High risk <5%),\n",
    "# the model might struggle to learn High risk patterns.\n",
    "# Solutions:\n",
    "#   - Collect more High risk examples\n",
    "#   - Use class weights in model.fit() to penalize High risk misclassification more\n",
    "#   - Oversample High risk sequences (SMOTE or duplicates)\n",
    "#   - Undersample Low risk sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM Model Architecture\n",
    "\n",
    "**Why Bi-LSTM?**\n",
    "- **LSTM (Long Short-Term Memory):** Designed for sequential data with long-term dependencies\n",
    "  - Has memory cells that selectively remember/forget information\n",
    "  - Can learn \"if student shows pattern X for 5 days, then likely pattern Y follows\"\n",
    "- **Bidirectional:** Processes sequences forward AND backward\n",
    "  - Forward: Looks at days 1→30 (\"how did we get here?\")\n",
    "  - Backward: Looks at days 30→1 (\"where are we going?\")\n",
    "  - Combined: Richer understanding of context\n",
    "\n",
    "**Architecture explained:**\n",
    "```\n",
    "Input: (batch_size, 30, 1)\n",
    "  ↓\n",
    "Bidirectional LSTM (128 units)\n",
    "  - Forward LSTM: Processes day 0→29\n",
    "  - Backward LSTM: Processes day 29→0\n",
    "  - Concatenates outputs → 256 features\n",
    "  ↓\n",
    "Dropout (50%)\n",
    "  - Randomly turns off 50% of neurons during training\n",
    "  - Prevents overfitting (model relying too heavily on specific patterns)\n",
    "  ↓\n",
    "Dense (64 neurons)\n",
    "  - Fully connected layer to combine LSTM features\n",
    "  - ReLU activation: f(x) = max(0, x)\n",
    "  ↓\n",
    "Dropout (30%)\n",
    "  - Additional regularization\n",
    "  ↓\n",
    "Dense (3 neurons, softmax)\n",
    "  - Output layer: 3 neurons for 3 classes (Low, Medium, High)\n",
    "  - Softmax: Converts outputs to probabilities summing to 1\n",
    "  - Example output: [0.1, 0.7, 0.2] → 70% Medium risk\n",
    "```\n",
    "\n",
    "**Key hyperparameters:**\n",
    "- **128 LSTM units:** Number of memory cells\n",
    "  - Too few (16): Model can't capture complex patterns\n",
    "  - Too many (512): Overfits, trains slowly, needs more data\n",
    "  - 128 is sweet spot for this problem size\n",
    "- **Dropout 0.5 and 0.3:** Regularization strength\n",
    "  - Higher dropout = more aggressive overfitting prevention\n",
    "  - We use higher dropout (0.5) after LSTM because it has more parameters\n",
    "- **Dense 64:** Intermediate layer size\n",
    "  - Compresses 256 LSTM features → 64 → 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(seq_len, n_features, n_classes):\n",
    "    \"\"\"\n",
    "    Build Bidirectional LSTM model for procrastination prediction.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Length of input sequences (e.g., 30 days)\n",
    "        n_features: Number of features per timestep (e.g., 1 for just clicks)\n",
    "        n_classes: Number of output classes (3 for Low/Med/High)\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model ready for training\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Input layer is implicit - defined by first layer\n",
    "        # Expected input shape: (batch_size, seq_len, n_features)\n",
    "        # batch_size is flexible (can be 32, 64, etc.)\n",
    "        \n",
    "        # Bidirectional LSTM layer\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                128,  # Number of LSTM units (memory cells)\n",
    "                return_sequences=False,  # Only return output from final timestep\n",
    "                # Why False? We want a single representation of the entire sequence\n",
    "                # If True, we'd get output for every timestep (30 outputs instead of 1)\n",
    "            ),\n",
    "            input_shape=(seq_len, n_features)  # Explicitly define input shape\n",
    "        ),\n",
    "        # Output shape: (batch_size, 256) because Bi-LSTM concatenates forward (128) + backward (128)\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        Dropout(0.5),  # Randomly drop 50% of connections during training\n",
    "        # Why high dropout? LSTM layers have many parameters and can easily overfit\n",
    "        # Output shape: (batch_size, 256) - dropout doesn't change shape, just randomly zeros values\n",
    "        \n",
    "        # Dense (fully connected) layer\n",
    "        Dense(\n",
    "            64,  # 64 neurons\n",
    "            activation='relu'  # ReLU: f(x) = max(0, x)\n",
    "            # ReLU is standard for hidden layers:\n",
    "            #   - Fast to compute\n",
    "            #   - Avoids vanishing gradient problem\n",
    "            #   - Introduces non-linearity (allows learning complex patterns)\n",
    "        ),\n",
    "        # Output shape: (batch_size, 64)\n",
    "        \n",
    "        # Another dropout layer\n",
    "        Dropout(0.3),  # Less aggressive than first dropout (30% vs 50%)\n",
    "        # Dense layer has fewer parameters, so needs less regularization\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(\n",
    "            n_classes,  # 3 neurons for Low/Medium/High\n",
    "            activation='softmax'  # Softmax converts to probabilities\n",
    "            # Softmax formula: softmax(x_i) = exp(x_i) / sum(exp(x_j) for all j)\n",
    "            # Example: Raw outputs [-1.5, 2.3, 0.8] → Probabilities [0.05, 0.71, 0.24]\n",
    "            # Properties:\n",
    "            #   - All outputs between 0 and 1\n",
    "            #   - Sum to 1.0 (valid probability distribution)\n",
    "            #   - Highest raw score gets highest probability\n",
    "        )\n",
    "        # Output shape: (batch_size, 3)\n",
    "        # Interpretation: [P(Low), P(Medium), P(High)] for each sequence\n",
    "    ])\n",
    "    \n",
    "    # Compile model with optimizer and loss function\n",
    "    model.compile(\n",
    "        optimizer='adam',  # Adam optimizer - adaptive learning rate\n",
    "        # Why Adam?\n",
    "        #   - Combines benefits of momentum and adaptive learning rates\n",
    "        #   - Works well with RNNs/LSTMs\n",
    "        #   - Default learning rate (0.001) is good starting point\n",
    "        #   - Requires minimal tuning\n",
    "        \n",
    "        loss='sparse_categorical_crossentropy',  # Loss function for multi-class classification\n",
    "        # Why sparse_categorical_crossentropy?\n",
    "        #   - \"categorical\" = multi-class classification (>2 classes)\n",
    "        #   - \"sparse\" = labels are integers (0, 1, 2), not one-hot vectors\n",
    "        #   - If labels were one-hot ([1,0,0], [0,1,0], [0,0,1]), use 'categorical_crossentropy'\n",
    "        # Formula: -sum(y_true * log(y_pred))\n",
    "        # Penalizes confident wrong predictions heavily\n",
    "        # Example: True label=High (2), predicted=[0.1, 0.2, 0.7] → low loss (correct)\n",
    "        #          True label=High (2), predicted=[0.7, 0.2, 0.1] → high loss (wrong)\n",
    "        \n",
    "        metrics=['accuracy']  # Track accuracy during training\n",
    "        # Accuracy = (correct predictions) / (total predictions)\n",
    "        # Simple but not always the best metric for imbalanced classes\n",
    "        # We'll also compute precision/recall later for deeper evaluation\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = build_model(\n",
    "    seq_len=X_train.shape[1],  # 30 (sequence length)\n",
    "    n_features=X_train.shape[2],  # 1 (just daily clicks)\n",
    "    n_classes=len(label_enc.classes_)  # 3 (Low/Medium/High)\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()\n",
    "\n",
    "# Understanding the summary:\n",
    "#\n",
    "# Layer (type)                Output Shape              Param #\n",
    "# =================================================================\n",
    "# bidirectional (Bidirection  (None, 256)              133120\n",
    "#   - \"None\" = batch size (flexible)\n",
    "#   - 256 = 2 * 128 units (forward + backward)\n",
    "#   - 133120 parameters = weights to learn\n",
    "#     Formula: 4 * ((n_features + units) * units + units)\n",
    "#     LSTM has 4 gates (input, forget, output, cell), each with its own weights\n",
    "#\n",
    "# dropout (Dropout)           (None, 256)              0\n",
    "#   - Dropout has no trainable parameters (just randomly zeros values)\n",
    "#\n",
    "# dense (Dense)               (None, 64)               16448\n",
    "#   - 16448 parameters = 256 inputs * 64 outputs + 64 biases\n",
    "#\n",
    "# dropout_1 (Dropout)         (None, 64)               0\n",
    "#\n",
    "# dense_1 (Dense)             (None, 3)                195\n",
    "#   - 195 parameters = 64 inputs * 3 outputs + 3 biases\n",
    "#\n",
    "# =================================================================\n",
    "# Total params: 149,763\n",
    "#   - These are the weights the model will learn during training\n",
    "# Trainable params: 149,763\n",
    "#   - All parameters are trainable (none are frozen)\n",
    "# Non-trainable params: 0\n",
    "#\n",
    "# Context: A model with 150k parameters is:\n",
    "# - Large enough to learn complex patterns\n",
    "# - Small enough to train in reasonable time on CPU\n",
    "# - Moderate overfitting risk (hence the dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "**What happens during training:**\n",
    "1. Model makes predictions on training data\n",
    "2. Loss function compares predictions to true labels\n",
    "3. Backpropagation calculates gradients (how to adjust weights to reduce loss)\n",
    "4. Optimizer updates weights\n",
    "5. Repeat for many epochs (passes through entire dataset)\n",
    "\n",
    "**Key training concepts:**\n",
    "- **Epoch:** One complete pass through the entire training dataset\n",
    "  - If training set has 100k sequences, one epoch processes all 100k\n",
    "  - We'll train for 50 epochs (can stop early if overfitting detected)\n",
    "- **Batch size:** Number of sequences processed before updating weights\n",
    "  - Batch size = 32 means: Process 32 sequences → calculate loss → update weights → repeat\n",
    "  - Smaller batches (16): Noisy gradients, more frequent updates, slower but less memory\n",
    "  - Larger batches (128): Smoother gradients, less frequent updates, faster but more memory\n",
    "  - 32 is a common default that balances these tradeoffs\n",
    "- **Validation split:** 20% of training data held out for validation\n",
    "  - Validation data is NOT used for training, only for monitoring\n",
    "  - Helps detect overfitting (training acc increases but validation acc decreases)\n",
    "\n",
    "**Callbacks:**\n",
    "- **EarlyStopping:** Stops training if validation loss stops improving\n",
    "  - Patience=10 means: If val_loss doesn't improve for 10 epochs, stop\n",
    "  - Saves time and prevents overfitting\n",
    "- **ModelCheckpoint:** Saves best model weights during training\n",
    "  - \"Best\" = lowest validation loss\n",
    "  - Without this, we'd only have final model (which might be overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training callbacks\n",
    "callbacks = [\n",
    "    # Early stopping callback\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',  # Metric to watch\n",
    "        # Why val_loss not val_accuracy?\n",
    "        #   - Loss is more sensitive to small changes\n",
    "        #   - Accuracy can plateau while loss still improves (better calibrated probabilities)\n",
    "        patience=10,  # Number of epochs with no improvement before stopping\n",
    "        # Example: If val_loss is [0.8, 0.75, 0.76, 0.76, 0.77, ...]\n",
    "        #          After 10 epochs without improvement from 0.75, stop training\n",
    "        restore_best_weights=True,  # Restore weights from best epoch\n",
    "        # Without this, model would keep the final epoch's weights (which might be worse)\n",
    "        verbose=1  # Print message when early stopping triggers\n",
    "    ),\n",
    "    \n",
    "    # Model checkpoint callback\n",
    "    ModelCheckpoint(\n",
    "        'best_model.h5',  # Filename to save weights\n",
    "        monitor='val_loss',  # Metric to determine \"best\"\n",
    "        save_best_only=True,  # Only save when model improves\n",
    "        # Alternative: save_best_only=False would save after every epoch (wastes storage)\n",
    "        mode='min',  # Save when val_loss decreases (lower is better)\n",
    "        # For accuracy, would use mode='max' (higher is better)\n",
    "        verbose=1  # Print message when saving\n",
    "    )\n",
    "]\n",
    "\n",
    "# Why these callbacks matter:\n",
    "# - Training deep learning models is expensive (hours/days for large datasets)\n",
    "# - Without early stopping, you might waste time training past the optimal point\n",
    "# - Without model checkpoint, you might lose the best model if training crashes\n",
    "# - Together, they make training robust and efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# This will take 30-60 minutes on Colab GPU, 2-4 hours on CPU\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,  # Training sequences\n",
    "    y_train,  # Training labels\n",
    "    validation_split=0.2,  # Use 20% of training data for validation\n",
    "    # This splits X_train into:\n",
    "    #   - 80% for actual training (updating weights)\n",
    "    #   - 20% for validation (monitoring, not used for training)\n",
    "    # Note: This is DIFFERENT from test set (which we haven't touched yet)\n",
    "    \n",
    "    epochs=50,  # Maximum number of passes through training data\n",
    "    # With early stopping, training might stop at epoch 20-30\n",
    "    \n",
    "    batch_size=32,  # Number of sequences per gradient update\n",
    "    # If training set has 80k sequences:\n",
    "    #   - 80k / 32 = 2500 batches per epoch\n",
    "    #   - Model updates weights 2500 times per epoch\n",
    "    \n",
    "    callbacks=callbacks,  # Early stopping and model checkpoint\n",
    "    \n",
    "    verbose=1  # Print progress bar and metrics for each epoch\n",
    "    # Epoch 1/50\n",
    "    # 2500/2500 [==============================] - 45s 18ms/step\n",
    "    # loss: 0.7234 - accuracy: 0.6812 - val_loss: 0.6543 - val_accuracy: 0.7123\n",
    ")\n",
    "\n",
    "# What to watch during training:\n",
    "# - loss and val_loss should both decrease\n",
    "# - accuracy and val_accuracy should both increase\n",
    "# - val_loss should stay close to loss (if diverging, model is overfitting)\n",
    "# - Training time per epoch (if >5 min/epoch on GPU, consider smaller model or batch size)\n",
    "\n",
    "# Common issues:\n",
    "# - loss not decreasing: Learning rate too high or too low, check data quality\n",
    "# - val_loss increases while loss decreases: Overfitting, add more dropout\n",
    "# - Both losses plateau early (e.g., at 0.9): Model too simple, increase units\n",
    "# - Very slow training: Use GPU (check runtime type in Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "**Finally, we test our model on truly unseen data (the test set).**\n",
    "\n",
    "**Important distinction:**\n",
    "- **Training set (64% of data):** Used to update model weights\n",
    "- **Validation set (16% of data):** Used to monitor during training, tune hyperparameters\n",
    "- **Test set (20% of data):** NEVER seen during training, used only for final evaluation\n",
    "\n",
    "**Metrics we'll calculate:**\n",
    "- **Accuracy:** Overall % correct (simple but incomplete)\n",
    "- **Precision:** When model predicts High risk, how often is it actually High?\n",
    "  - Important if false alarms are costly (annoying students with unnecessary interventions)\n",
    "- **Recall:** Of all actual High risk students, what % does model catch?\n",
    "  - Important if missing at-risk students is costly (they drop out without intervention)\n",
    "- **F1-score:** Harmonic mean of precision and recall (balanced metric)\n",
    "- **Confusion matrix:** Shows exactly which predictions were correct/wrong\n",
    "\n",
    "**Why test set matters:**\n",
    "Imagine training a model to 99% validation accuracy but only 60% test accuracy.\n",
    "This means the model memorized the validation set patterns (overfitting).\n",
    "The test set gives us an honest estimate of real-world performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "# verbose=0 suppresses progress bar (we just want final numbers)\n",
    "\n",
    "print(\"\\n=== Test Set Performance ===\")\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "# Loss context:\n",
    "#   - 0.0-0.5: Excellent (highly confident correct predictions)\n",
    "#   - 0.5-1.0: Good (confident predictions, some errors)\n",
    "#   - 1.0-1.5: Acceptable (less confident, more errors)\n",
    "#   - >1.5: Poor (random guessing would be ~1.1 for 3 classes)\n",
    "\n",
    "print(f\"Test Accuracy: {acc:.4f} ({acc:.1%})\")\n",
    "# Accuracy context for procrastination prediction:\n",
    "#   - >80%: Excellent (better than most published research on OULAD)\n",
    "#   - 70-80%: Good (usable for intervention systems)\n",
    "#   - 60-70%: Acceptable (better than random, but needs improvement)\n",
    "#   - <60%: Poor (not reliable enough for production)\n",
    "#   - 33%: Random guessing (1/3 for 3 classes)\n",
    "\n",
    "# Compare to validation performance:\n",
    "# If test accuracy is much lower than validation accuracy (e.g., val=85%, test=70%),\n",
    "# the model overfit to the validation set. Should increase regularization (dropout)\n",
    "# or collect more training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set\n",
    "y_pred_proba = model.predict(X_test)  # Get probability distributions\n",
    "# Shape: (num_test_sequences, 3)\n",
    "# Example: [[0.1, 0.7, 0.2], [0.8, 0.15, 0.05], ...]\n",
    "#          First sequence: 70% Medium, 20% High, 10% Low\n",
    "#          Second sequence: 80% Low, 15% Medium, 5% High\n",
    "\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)  # Convert to class labels\n",
    "# argmax finds index of maximum probability\n",
    "# Example: [0.1, 0.7, 0.2] → argmax = 1 (index of 0.7)\n",
    "# Result: [1, 0, ...] meaning [Medium, Low, ...]\n",
    "\n",
    "# Generate detailed classification report\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(\n",
    "    y_test,  # True labels\n",
    "    y_pred,  # Predicted labels\n",
    "    target_names=label_enc.classes_,  # ['High', 'Low', 'Medium']\n",
    "    digits=4  # Show 4 decimal places\n",
    "))\n",
    "\n",
    "# Understanding the classification report:\n",
    "#\n",
    "#               precision    recall  f1-score   support\n",
    "#\n",
    "#         High     0.7234    0.6543    0.6871      1234\n",
    "#          Low     0.8123    0.8432    0.8275      4567\n",
    "#       Medium     0.6789    0.7123    0.6952      2345\n",
    "#\n",
    "#     accuracy                         0.7654      8146\n",
    "#    macro avg     0.7382    0.7366    0.7366      8146\n",
    "# weighted avg     0.7689    0.7654    0.7669      8146\n",
    "#\n",
    "# Columns explained:\n",
    "# - precision: Of predictions for this class, how many were correct?\n",
    "#   High precision for Low = When we predict Low, we're usually right\n",
    "#   Formula: True Positives / (True Positives + False Positives)\n",
    "#\n",
    "# - recall: Of actual instances of this class, how many did we catch?\n",
    "#   High recall for High = We catch most High risk students\n",
    "#   Formula: True Positives / (True Positives + False Negatives)\n",
    "#\n",
    "# - f1-score: Harmonic mean of precision and recall\n",
    "#   F1 = 2 * (precision * recall) / (precision + recall)\n",
    "#   Balances precision and recall into single metric\n",
    "#\n",
    "# - support: Number of true instances of each class in test set\n",
    "#   Example: If support for High=1234, there are 1234 High risk sequences in test set\n",
    "#\n",
    "# Averages:\n",
    "# - macro avg: Simple average across classes (treats all classes equally)\n",
    "# - weighted avg: Average weighted by support (accounts for class imbalance)\n",
    "#   Use weighted avg as primary metric when classes are imbalanced\n",
    "#\n",
    "# What to look for:\n",
    "# - All classes should have f1-score > 0.60 (model learns all patterns, not just majority)\n",
    "# - High risk class is most important - check its recall\n",
    "#   If High recall < 0.60, model misses too many at-risk students\n",
    "# - Large gaps between classes suggest some patterns are easier to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and visualize confusion matrix\n",
    "# Confusion matrix shows exactly which classes are confused with each other\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Shape: (3, 3) for 3 classes\n",
    "# cm[i, j] = number of class i instances predicted as class j\n",
    "# Example:\n",
    "#        [[800  50  20]   ← 800 High correctly predicted as High\n",
    "#         [30  3000 100]   ← 3000 Low correctly predicted as Low\n",
    "#         [40  200 1800]]  ← 1800 Med correctly predicted as Med\n",
    "#    Diagonal = correct predictions\n",
    "#    Off-diagonal = errors\n",
    "\n",
    "# Visualize as heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,  # Show numbers in cells\n",
    "    fmt='d',  # Format as integers (not floats)\n",
    "    cmap='Blues',  # Color scheme (darker = more instances)\n",
    "    xticklabels=label_enc.classes_,  # Class names on x-axis\n",
    "    yticklabels=label_enc.classes_,  # Class names on y-axis\n",
    "    cbar_kws={'label': 'Count'}  # Label for colorbar\n",
    ")\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold', pad=15)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# How to interpret:\n",
    "#\n",
    "# Perfect model: All numbers on diagonal (no errors)\n",
    "# Reality: Some off-diagonal values (errors)\n",
    "#\n",
    "# Common error patterns:\n",
    "# 1. Medium often confused with Low/High (boundary cases)\n",
    "#    This is expected - procrastination is a spectrum\n",
    "# 2. High confused with Medium (underestimating risk)\n",
    "#    More dangerous - we miss at-risk students\n",
    "# 3. Low confused with High (overestimating risk)\n",
    "#    Less dangerous but causes unnecessary interventions\n",
    "#\n",
    "# Action items based on confusion matrix:\n",
    "# - If cm[High, Medium] is large (High predicted as Medium):\n",
    "#   → Model is not sensitive enough to High risk patterns\n",
    "#   → Solution: Oversample High risk, adjust class weights\n",
    "# - If cm[Medium, Low] ≈ cm[Medium, High]:\n",
    "#   → Medium class is genuinely hard to distinguish (borderline students)\n",
    "#   → This is acceptable and expected\n",
    "# - If off-diagonal values > diagonal (more errors than correct):\n",
    "#   → Model is performing worse than random, something is broken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training History Visualization\n",
    "\n",
    "**Purpose:** Understand how the model learned over time.\n",
    "\n",
    "**What we're plotting:**\n",
    "- **Loss curves:** How training and validation loss changed each epoch\n",
    "- **Accuracy curves:** How training and validation accuracy changed each epoch\n",
    "\n",
    "**Healthy training looks like:**\n",
    "- Both train and val loss decrease together\n",
    "- Both train and val accuracy increase together\n",
    "- Val metrics plateau slightly below train metrics (small gap is normal)\n",
    "\n",
    "**Warning signs:**\n",
    "- Train loss decreases but val loss increases → **Overfitting**\n",
    "  - Model memorizing training data instead of learning patterns\n",
    "  - Solution: More dropout, less model complexity, more training data\n",
    "- Both losses plateau early and stay high → **Underfitting**\n",
    "  - Model too simple to capture patterns\n",
    "  - Solution: More LSTM units, deeper architecture, more epochs\n",
    "- Unstable curves (jagged, oscillating) → **Learning rate too high**\n",
    "  - Model overshoots optimal weights\n",
    "  - Solution: Reduce learning rate (e.g., Adam with lr=0.0001 instead of 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy over epochs\n",
    "ax1.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Model Accuracy During Training', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# What to look for:\n",
    "# - Both curves should trend upward\n",
    "# - Val accuracy should be slightly below train accuracy (gap of 1-5% is normal)\n",
    "# - If val accuracy plateaus while train continues increasing → overfitting\n",
    "\n",
    "# Plot 2: Loss over epochs\n",
    "ax2.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "ax2.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Loss', fontsize=12)\n",
    "ax2.set_title('Model Loss During Training', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# What to look for:\n",
    "# - Both curves should trend downward\n",
    "# - Val loss should be slightly above train loss\n",
    "# - If val loss starts increasing while train decreases → stop training (early stopping should catch this)\n",
    "# - If curves are very jagged/noisy → might need larger batch size or lower learning rate\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional diagnostics:\n",
    "best_epoch = np.argmin(history.history['val_loss'])\n",
    "print(f\"\\nBest epoch: {best_epoch + 1}\")  # +1 because epochs start at 1 in display\n",
    "print(f\"Best val_loss: {history.history['val_loss'][best_epoch]:.4f}\")\n",
    "print(f\"Best val_accuracy: {history.history['val_accuracy'][best_epoch]:.4f}\")\n",
    "\n",
    "# If early stopping triggered:\n",
    "if len(history.history['loss']) < 50:\n",
    "    print(f\"\\nTraining stopped early at epoch {len(history.history['loss'])}\")\n",
    "    print(\"(Validation loss stopped improving)\")\n",
    "else:\n",
    "    print(\"\\nTraining completed all 50 epochs\")\n",
    "    print(\"(Consider increasing epochs if still improving)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Artifacts\n",
    "\n",
    "**Critical step:** Save everything needed for deployment.\n",
    "\n",
    "**What we need to save:**\n",
    "1. **Trained model** (best_model.h5 or procrastination_bilstm.h5)\n",
    "   - Contains learned weights (all 150k parameters)\n",
    "   - Can be loaded later to make predictions without retraining\n",
    "\n",
    "2. **Feature scaler** (scaler.pkl)\n",
    "   - Stores mean and std used for normalization\n",
    "   - MUST use same scaling for new data\n",
    "   - Example: If training irregularity had mean=1.5, std=1.0,\n",
    "     new student with irregularity=2.5 must be scaled to (2.5-1.5)/1.0 = 1.0\n",
    "\n",
    "3. **Label encoder** (label_encoder.pkl)\n",
    "   - Maps integers back to text labels\n",
    "   - Example: Model outputs [0.1, 0.2, 0.7] → argmax = 2 → label_enc.inverse_transform([2]) = 'Medium'\n",
    "\n",
    "4. **Feature dataframe with labels** (features_with_labels.csv)\n",
    "   - For analysis, visualization, fine-tuning\n",
    "   - Includes all engineered features and cluster assignments\n",
    "\n",
    "**Deployment workflow:**\n",
    "```python\n",
    "# When deploying to production:\n",
    "model = keras.models.load_model('procrastination_bilstm.h5')\n",
    "scaler = pickle.load(open('scaler.pkl', 'rb'))\n",
    "label_enc = pickle.load(open('label_encoder.pkl', 'rb'))\n",
    "\n",
    "# For new student:\n",
    "new_features = calculate_features(new_student_data)\n",
    "new_features_scaled = scaler.transform(new_features)  # Use SAME scaler\n",
    "new_sequence = create_sequence(new_student_data, seq_len=30)\n",
    "prediction = model.predict(new_sequence)\n",
    "risk_label = label_enc.inverse_transform([np.argmax(prediction)])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "model.save('procrastination_bilstm.h5')\n",
    "# HDF5 format (.h5) saves:\n",
    "#   - Model architecture (layers, connections)\n",
    "#   - Model weights (all 150k learned parameters)\n",
    "#   - Optimizer state (for resuming training if needed)\n",
    "#   - Compilation configuration (loss, metrics)\n",
    "print(\"✓ Model saved to procrastination_bilstm.h5\")\n",
    "\n",
    "# Alternative: TensorFlow SavedModel format (for deployment)\n",
    "# model.save('procrastination_model', save_format='tf')\n",
    "# Benefits: Better for TensorFlow Serving, TensorFlow Lite, TensorFlow.js\n",
    "# Drawback: Creates directory instead of single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features with cluster labels\n",
    "features_clean.to_csv('features_with_labels.csv', index=False)\n",
    "# index=False: Don't save row numbers as a column\n",
    "print(\"✓ Features saved to features_with_labels.csv\")\n",
    "\n",
    "# This CSV contains:\n",
    "# - All engineered features (late_rate, irregularity, etc.)\n",
    "# - Cluster assignments (0, 1, 2)\n",
    "# - Risk labels (Low, Medium, High)\n",
    "# - Student identifiers (id_student, code_module, code_presentation)\n",
    "#\n",
    "# Uses:\n",
    "# - Analyze which features matter most\n",
    "# - Visualize cluster characteristics\n",
    "# - Fine-tune model on local data (add new students to this CSV)\n",
    "# - Validate predictions (compare model output to cluster labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessing objects (scaler and label encoder)\n",
    "import pickle\n",
    "\n",
    "# Save scaler\n",
    "with open('scaler.pkl', 'wb') as f:  # 'wb' = write binary\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"✓ Scaler saved to scaler.pkl\")\n",
    "\n",
    "# Save label encoder\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_enc, f)\n",
    "print(\"✓ Label encoder saved to label_encoder.pkl\")\n",
    "\n",
    "# Why pickle?\n",
    "# - Pickle serializes Python objects to files\n",
    "# - Preserves exact state (mean, std, class mappings)\n",
    "# - Can be loaded in any Python environment\n",
    "#\n",
    "# IMPORTANT: scaler and label_enc MUST match training data\n",
    "# If you retrain the model, MUST also recreate and resave these objects\n",
    "# Using old scaler with new model will give wrong predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we've accomplished:**\n",
    "\n",
    "1. ✅ **Loaded and explored OULAD** - 32k students, 10M+ interactions\n",
    "2. ✅ **Engineered procrastination features** - 8 behavioral indicators\n",
    "3. ✅ **Created risk labels via K-Means** - Low/Medium/High clusters\n",
    "4. ✅ **Validated temporal structure** - ACF/PACF analysis, stationarity test\n",
    "5. ✅ **Generated sequences** - Converted clickstreams to LSTM-compatible format\n",
    "6. ✅ **Built Bi-LSTM model** - 150k parameters, bidirectional architecture\n",
    "7. ✅ **Trained with regularization** - Dropout, early stopping, best model checkpoint\n",
    "8. ✅ **Evaluated performance** - Test accuracy, confusion matrix, classification report\n",
    "9. ✅ **Saved artifacts** - Model, scaler, encoder, features for deployment\n",
    "\n",
    "**This pre-trained model is ready for:**\n",
    "- **Direct deployment:** Make predictions on new OULAD students\n",
    "- **Transfer learning:** Fine-tune on local institutional data (ALU, ASSISTments, Canvas)\n",
    "- **Multi-window training:** Create 3-day, 7-day, 14-day variants for your 1-week testing\n",
    "\n",
    "**Next steps for YOUR capstone:**\n",
    "1. Train 3-day and 7-day models (modify seq_len parameter)\n",
    "2. Compare accuracy across different sequence lengths\n",
    "3. Deploy 7-day model in your web platform\n",
    "4. Collect behavioral data from ALU students during pilot test\n",
    "5. Fine-tune model on local data (transfer learning)\n",
    "6. Measure if transfer learning improves accuracy over pure OULAD model\n",
    "\n",
    "**Model is ready for transfer learning with local institutional data!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "L4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
