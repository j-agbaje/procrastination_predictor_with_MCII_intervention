{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procrastination Prediction using OULAD Dataset\n",
    "**Author:** Jeremiah Agbaje  \n",
    "**Date:** February 2026\n",
    "\n",
    "Pre-training a Bi-LSTM model on OULAD behavioral data for transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn tensorflow statsmodels -q"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score\n",
    "from sklearn.decomposition import PCA"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU: {tf.config.list_physical_devices('GPU')}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "BASE_PATH = '/content/drive/MyDrive/ALU Capstone/OULAD_data/'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Loading datasets...\")\n",
    "student_vle = pd.read_csv(BASE_PATH + 'studentVle.csv')\n",
    "student_assessment = pd.read_csv(BASE_PATH + 'studentAssessment.csv')\n",
    "student_info = pd.read_csv(BASE_PATH + 'studentInfo.csv')\n",
    "assessments = pd.read_csv(BASE_PATH + 'assessments.csv')\n",
    "vle = pd.read_csv(BASE_PATH + 'vle.csv')\n",
    "\n",
    "print(f\"VLE interactions: {len(student_vle):,}\")\n",
    "print(f\"Assessments: {len(student_assessment):,}\")\n",
    "print(f\"Students: {len(student_info):,}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "student_vle.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "student_assessment.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "student_info.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Missing values:\")\n",
    "print(student_vle.isnull().sum())\n",
    "print(\"\\n\", student_assessment.isnull().sum())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data for Faster Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "SAMPLE_SIZE = 5000\n",
    "student_sample = student_info.sample(n=min(SAMPLE_SIZE, len(student_info)), random_state=42)\n",
    "student_ids = student_sample['id_student'].unique()\n",
    "\n",
    "vle_sample = student_vle[student_vle['id_student'].isin(student_ids)]\n",
    "assess_sample = student_assessment[student_assessment['id_student'].isin(student_ids)]\n",
    "\n",
    "print(f\"Sampled {len(student_sample)} students\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Creating behavioral indicators for procrastination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def engineer_features(vle_data, assess_data, assess_info, student_data):\n",
    "    features = []\n",
    "    \n",
    "    unique_students = student_data[['code_module', 'code_presentation', 'id_student']].drop_duplicates()\n",
    "    \n",
    "    for idx, (module, presentation, sid) in enumerate(unique_students.values):\n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"Processing {idx}/{len(unique_students)}...\")\n",
    "        \n",
    "        s_vle = vle_data[\n",
    "            (vle_data['code_module'] == module) & \n",
    "            (vle_data['code_presentation'] == presentation) & \n",
    "            (vle_data['id_student'] == sid)\n",
    "        ]\n",
    "        \n",
    "        s_assess = assess_data[assess_data['id_student'] == sid]\n",
    "        s_assess_full = s_assess.merge(assess_info[['id_assessment', 'date']], on='id_assessment', how='left')\n",
    "        \n",
    "        if len(s_assess_full) > 0:\n",
    "            s_assess_full['days_late'] = s_assess_full['date_submitted'] - s_assess_full['date']\n",
    "            late_rate = (s_assess_full['days_late'] > 0).sum() / len(s_assess_full)\n",
    "            avg_late = s_assess_full['days_late'].mean()\n",
    "        else:\n",
    "            late_rate = avg_late = 0\n",
    "        \n",
    "        if len(s_vle) > 0:\n",
    "            daily = s_vle.groupby('date')['sum_click'].sum()\n",
    "            irregularity = daily.std() / daily.mean() if daily.mean() > 0 else 0\n",
    "            total_clicks = daily.sum()\n",
    "            active_days = len(daily)\n",
    "            \n",
    "            dates = sorted(s_vle['date'].unique())\n",
    "            gaps = np.diff(dates) if len(dates) > 1 else [0]\n",
    "            avg_gap = gaps.mean()\n",
    "            max_gap = gaps.max()\n",
    "        else:\n",
    "            irregularity = total_clicks = active_days = avg_gap = max_gap = 0\n",
    "        \n",
    "        last_min_ratio = 0\n",
    "        if len(s_vle) > 0 and len(s_assess_full) > 0:\n",
    "            for deadline in s_assess_full['date'].dropna():\n",
    "                week_clicks = s_vle[\n",
    "                    (s_vle['date'] >= deadline - 7) & \n",
    "                    (s_vle['date'] <= deadline)\n",
    "                ]['sum_click'].sum()\n",
    "                last_min_ratio += week_clicks\n",
    "            last_min_ratio = last_min_ratio / total_clicks if total_clicks > 0 else 0\n",
    "        \n",
    "        features.append({\n",
    "            'id_student': sid,\n",
    "            'code_module': module,\n",
    "            'code_presentation': presentation,\n",
    "            'late_rate': late_rate,\n",
    "            'avg_days_late': avg_late,\n",
    "            'irregularity': irregularity,\n",
    "            'last_min_ratio': last_min_ratio,\n",
    "            'avg_gap': avg_gap,\n",
    "            'max_gap': max_gap,\n",
    "            'total_clicks': total_clicks,\n",
    "            'active_days': active_days,\n",
    "            'num_assessments': len(s_assess_full)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(features)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Engineering features...\")\n",
    "features_df = engineer_features(vle_sample, assess_sample, assessments, student_sample)\n",
    "print(f\"Created {len(features_df)} feature vectors\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "features_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "features_clean = features_df[\n",
    "    (features_df['num_assessments'] > 0) & \n",
    "    (features_df['total_clicks'] > 0)\n",
    "].copy()\n",
    "\n",
    "print(f\"Valid students: {len(features_clean)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "features_to_plot = ['late_rate', 'avg_days_late', 'irregularity', 'last_min_ratio', 'avg_gap', 'max_gap']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feat in enumerate(features_to_plot):\n",
    "    axes[idx].hist(features_clean[feat].fillna(0), bins=40, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(feat.replace('_', ' ').title())\n",
    "    axes[idx].set_xlabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "features_clean[features_to_plot].describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "corr_matrix = features_clean[features_to_plot].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "Creating procrastination labels from behavioral patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "cluster_features = ['late_rate', 'irregularity', 'last_min_ratio', 'avg_gap']\n",
    "X_cluster = features_clean[cluster_features].fillna(0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_cluster)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "inertias = []\n",
    "silhouettes = []\n",
    "K_range = range(2, 8)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_scaled, labels))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(K_range, inertias, 'bo-')\n",
    "ax1.set_xlabel('Number of Clusters')\n",
    "ax1.set_ylabel('Inertia')\n",
    "ax1.set_title('Elbow Method')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(K_range, silhouettes, 'ro-')\n",
    "ax2.set_xlabel('Number of Clusters')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Analysis')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "optimal_k = 3\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "features_clean['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(features_clean['cluster'].value_counts().sort_index())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "cluster_summary = features_clean.groupby('cluster')[cluster_features].mean()\n",
    "cluster_summary"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "risk_scores = cluster_summary[['late_rate', 'irregularity', 'last_min_ratio']].mean(axis=1)\n",
    "cluster_order = risk_scores.sort_values().index.tolist()\n",
    "\n",
    "risk_map = {cluster_order[0]: 'Low', cluster_order[1]: 'Medium', cluster_order[2]: 'High'}\n",
    "features_clean['risk'] = features_clean['cluster'].map(risk_map)\n",
    "\n",
    "print(features_clean['risk'].value_counts())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "for cluster in features_clean['cluster'].unique():\n",
    "    mask = features_clean['cluster'] == cluster\n",
    "    risk = risk_map[cluster]\n",
    "    ax1.scatter(X_pca[mask, 0], X_pca[mask, 1], label=f'{risk} Risk', alpha=0.6, s=30)\n",
    "\n",
    "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax1.set_title('Student Clusters (PCA)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "risk_counts = features_clean['risk'].value_counts()\n",
    "colors = {'Low': '#2ecc71', 'Medium': '#f39c12', 'High': '#e74c3c'}\n",
    "ax2.bar(risk_counts.index, risk_counts.values, color=[colors[x] for x in risk_counts.index])\n",
    "ax2.set_xlabel('Risk Level')\n",
    "ax2.set_ylabel('Students')\n",
    "ax2.set_title('Risk Distribution')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis\n",
    "\n",
    "Checking if LSTM is appropriate for this sequential data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sample_student = features_clean.iloc[0]\n",
    "student_clicks = vle_sample[\n",
    "    (vle_sample['id_student'] == sample_student['id_student']) &\n",
    "    (vle_sample['code_module'] == sample_student['code_module']) &\n",
    "    (vle_sample['code_presentation'] == sample_student['code_presentation'])\n",
    "]\n",
    "\n",
    "daily_clicks = student_clicks.groupby('date')['sum_click'].sum().sort_index()\n",
    "\n",
    "print(f\"Analyzing student {sample_student['id_student']}\")\n",
    "print(f\"Days of activity: {len(daily_clicks)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(daily_clicks.values)\n",
    "plt.title('Daily Click Pattern (Sample Student)')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Clicks')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "adf_result = adfuller(daily_clicks.values)\n",
    "print(f\"ADF Statistic: {adf_result[0]:.4f}\")\n",
    "print(f\"p-value: {adf_result[1]:.4f}\")\n",
    "print(f\"Stationary: {adf_result[1] < 0.05}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "plot_acf(daily_clicks.values, lags=30, ax=ax1)\n",
    "ax1.set_title('Autocorrelation Function (ACF)')\n",
    "\n",
    "plot_pacf(daily_clicks.values, lags=30, ax=ax2)\n",
    "ax2.set_title('Partial Autocorrelation Function (PACF)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Generation for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_sequences(vle_data, features_data, seq_len=30):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    label_enc = LabelEncoder()\n",
    "    features_data['risk_enc'] = label_enc.fit_transform(features_data['risk'])\n",
    "    \n",
    "    for idx, row in features_data.iterrows():\n",
    "        if idx % 500 == 0:\n",
    "            print(f\"{idx}/{len(features_data)}\")\n",
    "        \n",
    "        student_data = vle_data[\n",
    "            (vle_data['id_student'] == row['id_student']) &\n",
    "            (vle_data['code_module'] == row['code_module']) &\n",
    "            (vle_data['code_presentation'] == row['code_presentation'])\n",
    "        ]\n",
    "        \n",
    "        if len(student_data) < seq_len:\n",
    "            continue\n",
    "        \n",
    "        daily = student_data.groupby('date')['sum_click'].sum().sort_index()\n",
    "        \n",
    "        for i in range(len(daily) - seq_len):\n",
    "            seq = daily.iloc[i:i+seq_len].values\n",
    "            seq_norm = seq / seq.max() if seq.max() > 0 else seq\n",
    "            sequences.append(seq_norm.reshape(-1, 1))\n",
    "            labels.append(row['risk_enc'])\n",
    "    \n",
    "    return np.array(sequences), np.array(labels), label_enc"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Creating sequences...\")\n",
    "X_seq, y_labels, label_enc = create_sequences(vle_sample, features_clean, seq_len=30)\n",
    "\n",
    "print(f\"X shape: {X_seq.shape}\")\n",
    "print(f\"y shape: {y_labels.shape}\")\n",
    "print(f\"Classes: {label_enc.classes_}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq, y_labels, test_size=0.2, random_state=42, stratify=y_labels\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} samples\")\n",
    "print(f\"Test: {X_test.shape[0]} samples\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for cls, cnt in zip(label_enc.classes_, counts):\n",
    "    print(f\"{cls}: {cnt} ({cnt/len(y_train)*100:.1f}%)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def build_model(seq_len, n_features, n_classes):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(128, return_sequences=True), input_shape=(seq_len, n_features)),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(32, return_sequences=False)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "model = build_model(\n",
    "    seq_len=X_train.shape[1],\n",
    "    n_features=X_train.shape[2],\n",
    "    n_classes=len(np.unique(y_train))\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {acc:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "print(classification_report(y_test, y_pred, target_names=label_enc.classes_))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=label_enc.classes_,\n",
    "           yticklabels=label_enc.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(history.history['accuracy'], label='Train')\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation')\n",
    "ax1.set_title('Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(history.history['loss'], label='Train')\n",
    "ax2.plot(history.history['val_loss'], label='Validation')\n",
    "ax2.set_title('Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "model.save('procrastination_bilstm.h5')\n",
    "print(\"Model saved\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "features_clean.to_csv('features_with_labels.csv', index=False)\n",
    "print(\"Features saved\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_enc, f)\n",
    "\n",
    "print(\"Preprocessors saved\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Pre-trained model ready for transfer learning with local institutional data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
